{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.神经网络(`Neural Networks`)\n",
    "不妨先简单地将神经网络理解为：能自动从数据中学习到合适的权重的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 1.分类问题\n",
    "神经网络是机器学习中的一种模型，可以用于两类问题的解答：\n",
    "   - 分类：把数据划分成不同的类别\n",
    "   - 回归：建立数据间的连续关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.感知机(`Perceptron`)\n",
    "- 一般而言，`朴素感知机`是指单层网络，指的是激活函数使用了阶跃函数的模型；`多层感知机`是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。\n",
    "---\n",
    "1. 感知机接收多个输入，输出一个结果。其结果只有`Positive/Negative(1/0)`两种取值。通常将感知机的方程用向量法简写为：$$Wx + b = 0\\\\W = (w_{1}, w_{2}, \\cdots)\\\\x = (x_{1}, x_{2}, \\cdots)\\\\y = label:0 or 1))\\\\PREDICTION:\\\\ \\hat{y} = \\begin{cases} 1, &Wx + b \\geq0\\\\ 0, &Wx + b < 0\\end{cases}$$\n",
    "   - 对于更高维度(具有多于两个特征)的模型，一样是可以简化成：$$Wx + b = 0$$的方程\n",
    "   - 激活函数(`activation function`)：将输入信号的总和转换成输出信号的函数。激活函数是连接感知机和神经网络的桥梁。\n",
    "   - 阶跃函数(`step function`)：激活函数以阈值为界，一旦输入超过阈值，就切换输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.感知机算法的实现\n",
    "1. 用随机的权重$w_{1},w_{2}, \\cdots, w_{n}$初始化，得到方程$Wx + b = 0$和正负区域；\n",
    "2. 找到$x_{1}, x_{2}, \\cdots, x_{n}$中所有的分类错误点，然后遍历循环执行以下操作：\n",
    "   - if prediction = 0:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} + \\alpha * x_{i}$\n",
    "      - $b = b + \\alpha$\n",
    "   - if prediction = 1:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} - \\alpha * x_{i}$\n",
    "      - $b = b - \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.误差函数(`Error Function`)\n",
    "1. 误差函数提供给我们的预测值与实际值之间的差异，通过寻找最小的误差函数值来找到与实际值误差最小的预测值，从而更新权重。\n",
    "2. 在简单的线性方程中，我们可以通过判断“预测值与实测值相比是大了还是小了”来决定权重是增加还是减少。但是在更为复杂的非线性环境中呢？答案至一就是：\n",
    "3. 梯度下降法(`gradient descent method`)：通过不断地沿梯度方向前进，逐渐减小函数值的过程。\n",
    "4. 对于优化，误差函数选择连续型(`continued`)比离散型(`discrete`)要好。需要从离散型预测变成连续型预测。离散型得到就是`yes`或`no`(1/0)，而连续型得到的将是一个介于(0,1)之间的数字，我们可以视之为概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.Sigmoid函数\n",
    "$$\n",
    "h(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "---\n",
    "1. sigmoid函数与阶跃函数的不同：\n",
    "    - `平滑性`的不同。sigmoid是一条平滑的曲线，输出随着输入发生连续的变化；而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。\n",
    "    - sigmoid函数可以返回(0,1)之间的实数，而阶跃函数只能返回(0/1)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.Softmax函数\n",
    "$$\n",
    "y_{k} = \\frac{e^{a_{k}}}{\\sum_{i=0}^{n} e^{a_{i}}}\n",
    "$$\n",
    "---\n",
    "1. softmax函数的输出受到每个输入信号的影响。\n",
    "2. softmax函数的缺点：其在实现中要进行指数运算，但是此时指数函数的值很容易变得非常大，很容易造成`溢出问题`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.One-Hot 编码\n",
    "    计算机在表示多结果的分类时，使用One-Hot编码是比较常见的处理方式。\n",
    "---\n",
    "![应用示例](./one_hot_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.最大似然率(`Maximum Likeihood`)\n",
    "    一组事件，每个事件在模型中真实发生的概率的乘积。通过比较不同的模型的概率乘积，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.交叉熵(`Cross Entropy`)\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n} y * \\ln^{p} + (1-y) * \\ln^{1-p}\n",
    "$$\n",
    "---\n",
    "1. 对8中所提到的每个事件发生的概率求其ln值，再求其相反数的和，这个和就是交叉熵。\n",
    "2. 交叉熵越小，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.多分类交叉熵\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}*\\ln^{p_{ij}}\n",
    "$$\n",
    "---\n",
    "![应用示例](./muti-class_cross-entropy_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 11.Logistic(对数几率)回归\n",
    "    对数几率算法是所有机器学习的基石\n",
    "---\n",
    "1. 基本流程\n",
    "    - 获得数据\n",
    "    - 选择一个随机模型\n",
    "    - 计算误差\n",
    "    - 最小化误差，获得更好的模型\n",
    "    - 完成！\n",
    "2. 计算误差函数\n",
    "$$\n",
    "error = -y * \\ln^{\\hat{y}} - (1-y) * \\ln^{1-\\hat{y}}\n",
    "$$\n",
    "$$\n",
    "error-function  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\hat{y_{i}}} - (1-y_{i}) * \\ln^{1-\\hat{y_{i}}}\n",
    "$$\n",
    "---\n",
    "PS：图中的log 应为 ln；左侧公式需改为$-\\ln^{0.6} - \\ln^{0.2} - \\ln^{0.1} - \\ln^{0.7}$ = 4.8，并非log(0.6)= - (log0.2) - log(0.1) - log(0.7) = 4.8 ；右侧公式需改为$-\\ln^{0.7} - \\ln^{0.9} - \\ln^{0.9} - \\ln^{0.6}$ = 1.2，并非log(0.7)= - (log0.9) - log(0.9) - log(0.6) = 1.2\n",
    "![应用示例](./logistic_error_sample.jpg)\n",
    "由于$\\hat{y}$是线性方程$Wx + b$的sigmoid函数，误差函数可以写成：\n",
    " $$\n",
    "E(W, b)  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\sigma(Wx^{(i)} + b)} - (1-y_{i}) * \\ln^{1-\\sigma(Wx^{(i)} + b)}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 12.梯度下降的推导过程与实现\n",
    "- 梯度，是误差函数关于权重$(W_{1}, W_{2},\\cdots, W_{n})$和偏差的偏导数所形成的向量。\n",
    "- 梯度下降法就是按照高度(`误差函数`)的梯度负值方向，移动很多次，每一次叫做`epoch`\n",
    "![梯度的实际意义](./gradient_descent_define.jpg)\n",
    "- 多层感知机的梯度计算方法是一样的，只是$\\hat{y}$的方程更加复杂,梯度也几乎一样，只是表达式长了很多：$$\\nabla E_{i,j} = \\frac{\\partial E}{\\partial w_{j}^{(i)}}$$\n",
    "\n",
    "- 推导过程：\n",
    "    首先，sigmoid函数具有完美的导数，即\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1-\\sigma(x))\n",
    "$$\n",
    "  简单推导下：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ba81c06c-40be-4ae9-b557-cc0f74cd4116\" width=\"251px\" class=\"index--image--1wh9w\">\n",
    "现在，如果有$m$个样本点，标为$x^{(1)}, x^{(2)}, \\cdots, x^{(m)}$, 误差公式是：\n",
    "$$\n",
    "E = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\ln(\\hat{y^{(i)}}) + (1-y^{(i)}) \\ln (1-\\hat{y^{(i)}}) \\right)\n",
    "$$\n",
    "预测是$\\hat{y^{(i)}} = \\sigma(Wx^{(i)} + b)$。<br>我们的目标是计算$E$, 在单个样本点 x 时的梯度（偏导数），其中 x 包含 n 个特征，即$x = (x_{1}, \\ldots, x_n)$。\n",
    "$$\n",
    "\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)\n",
    "$$\n",
    "为此，首先我们要计算$\\frac{\\partial}{\\partial w_j} \\hat{y}$.<br>\n",
    "$\\hat{y} = \\sigma(Wx+b),$因此：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cfe9e171-2608-4c05-a1bb-f9a7d1a5eee1\" width=\"752px\" class=\"index--image--1wh9w\">\n",
    "最后一个等式是因为和中的唯一非常量项相对于$w_{j}$正好是$w_{j}x_{j}$,明显具有导数$x_{j}$.<br>\n",
    "现在可以计算$\\frac{\\partial}{\\partial w_j} E$.\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ccfebc74-13ff-48a8-9d8c-3562f5b9945b\" width=\"780px\" class=\"index--image--1wh9w\">\n",
    "类似的计算将得出：<br>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/936e53ac-6b05-436e-bbc9-9f5a01e82a0a\" width=\"185px\" class=\"index--image--1wh9w\">\n",
    "这个实际上告诉了我们很重要的规则。对于具有坐标$(x_1, \\ldots, x_n)$的点，标签$y$, 预测$\\hat{y}$,该点的误差函数梯度是$\\left(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}) \\right)$.<br>\n",
    "总之\n",
    "$$\n",
    "\\nabla E(W,b) = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1).\n",
    "$$\n",
    "如果思考下，会发现很神奇。梯度实际上是标量乘以点的坐标！什么是标量？也就是标签和预测直接的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。\n",
    "![推导过程](./gradient_descent_algorithm_how.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 13.对比Logistic(对数概率)感知器和梯度下降\n",
    "区别在于梯度下降的$\\hat{y}$可以取到(0，1)之间的实数，对数概率的$\\hat{y}$只能取到(0/1).\n",
    "---\n",
    "![对比图](./gradient_descent_algorithm_and_logistic_algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 二.深度神经网络(`Deep Neural Networks`)\n",
    "简单来说，就是多个感知机的叠加，但是输出层可能有多个节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.多层感知机(神经网络架构)\n",
    "1. 多层感知机，顾名思义就是一层一层朴素感知机的叠加，第一层的输入经过其方程处理后得到一个输出，这个输出是第一层的输出，也是第二层的输入，最终我们通过一个激活函数得到最终的输出结果。\n",
    "![多层感知机结构图](./deep_neural_networks_structure.jpg)<br>\n",
    "2. 多层感知机在处理二分类问题时，就如上述所说，那么如果处理多分类问题呢？其实也差不多。我们可以在输出层添加更多的节点，每个节点输出各自的得分，再使用之前提到的Softmax函数算出各自的概率。\n",
    "![多层感知机多分类结构图](./deep_neural_networks_muti_class_structure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.前向反馈(`Feedforward`)\n",
    "1. 表示的是从输入到输出的传递处理。\n",
    "2. 传入输入向量，应用线性模型序列和sigmoid函数，每一层的线性特征图相结合，最终变成高度非线性特征图。最终公式为：$$\\hat y = \\sigma(W^{(2)}\\sigma(W^{(1)}*x))$$\n",
    "![信号传递](./forward_matrix.jpg)\n",
    "图中$W^{(1)}$是3x2矩阵，$x$是3x1矩阵，无法直接相乘，应该使用$W^{(1)}$的转置与$x$相乘<br>\n",
    "3. 多层感知机的前向反馈\n",
    "![多层信号传递](./forward_multi_layer_matrix.jpg)\n",
    "4. 神经网络的误差函数\n",
    "    误差函数依旧可以使用之前的方程，只是$\\hat{y}$的形势更加复杂一些。$$\\hat y = \\sigma(W^{(3)}\\sigma(W^{(2)}\\sigma(W^{(1)}*x)))$$$$E  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\hat{y_{i}}} - (1-y_{i}) * \\ln^{1-\\hat{y_{i}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.后向传播(`Back propagation`)\n",
    "1. 后向反馈包括：\n",
    "    - 进行前向反馈运算。\n",
    "    - 将模型的输出与期望的输出进行比较。\n",
    "    - 计算误差。\n",
    "    - 向后运行前向反馈运算（反向传播），将误差分散到每个权重上。\n",
    "    - 更新权重，并获得更好的模型。\n",
    "    - 继续此流程，直到获得很好的模型。\n",
    "2. 多层感知机的后向传播和朴素感知机的过程是一样的，只是误差函数更加复杂一些。\n",
    "3. 对于多层感知机$\\nabla E$向量的每一项，将$\\frac{\\partial E}{\\partial w_{ij}^{(k)}}$乘以学习速率$\\alpha$，这样就得到了更新后的$W_{ij}^{'(k)}$\n",
    "4. 前向反馈是多种函数的复合，而后向反馈是算出每一步的导数，而根据链式法则，计算复合函数的导数=每一层函数的偏导数相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.用 Keras 构建神经网络\n",
    "要使用 Keras，你需要知道以下几个核心概念。\n",
    "- 序列模型\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    #Create the Sequential model\n",
    "    model = Sequential()\n",
    "```\n",
    "<a target=\"_blank\" href=\"https://keras.io/models/sequential/\">keras.models.Sequential</a> 类是神经网络模型的封装容器。它会提供常见的函数，例如 <code>fit()</code>、<code>evaluate()</code>和<code>compile()</code>。\n",
    "- 层<br>\n",
    "Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 <code>add()</code> 函数添加层。\n",
    "   - 全连接层(`fully-connected`)：相邻层的所有神经元之间都有连接。\n",
    "   - 最大池化层\n",
    "   - 激活层<br>\n",
    "简单的模型可以如下所示：\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "    #创建序列模型\n",
    "    model = Sequential()\n",
    "\n",
    "    #第一层 - 添加有128个节点的全连接层以及32个节点的输入层\n",
    "    model.add(Dense(128, input_dim=32))\n",
    "\n",
    "    #第二层 - 添加 softmax 激活层\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #第三层 - 添加全连接层\n",
    "    model.add(Dense(10))\n",
    "\n",
    "    #第四层 - 添加 Sigmoid 激活层\n",
    "    model.add(Activation('sigmoid'))\n",
    "```\n",
    "Keras 将根据第一层自动推断后续所有层的形状。这意味着，你只需为第一层设置输入维度。<br>\n",
    "上面的第一层<code>model.add(Dense(input_dim=32))</code> 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。<br>\n",
    "构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 <code>categorical_crossentropy</code>。我们还可以指定优化程序，稍后我们将了解这一概念，暂时将使用 <code>adam</code>。最后，我们可以指定评估模型用到的指标。我们将使用准确率。\n",
    "```python\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "```\n",
    "我们可以使用以下命令来查看模型架构：\n",
    "```python\n",
    "    model.summary()\n",
    "```\n",
    "然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。<br>\n",
    "然后使用fit命令训练模型并通过 epoch 参数来指定训练轮数（周期），每 epoch 完成对整数据集的一次遍历。 verbose 参数可以指定显示训练过程信息类型，这里定义为 0 表示不显示信息。\n",
    "```python\n",
    "    model.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "```\n",
    "注意：在 Keras 1 中，<code>nb_epoch</code>会设置 epoch 次数，但是在 Keras 2 中，变成了<code>epochs</code>。<br>\n",
    "最后，我们可以使用以下命令来评估模型：\n",
    "```python\n",
    "    model.evaluate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.早期停止(`Early stopping`)\n",
    "对模型使用梯度下降，直到测试误差停止降低并开始增大，此时停止，这就是早期停止法，广泛应用于训练神经网络。\n",
    "![模型复杂度图表](./early_stopping_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.正则化(`Regularization`)\n",
    "1. 正则化方法的出现，是为了应对过拟合问题。具体的方法有：权值衰减，Dropout等。\n",
    "2. 权值衰减(`Overfit weight decay`)\n",
    "   - 该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。因为很多过拟合就是因为权重参数取值过大而造成。\n",
    "   - 神经网络的学习目的就是为了减小损失函数的值，此时在损失函数加上权重的平方范数(`L2范数`)，这样就可以抑制权重变大。\n",
    "   - 对于权重$W$，L2范数的权重衰减就是$\\frac{1}{2}\\lambda W^{2}$($\\lambda$是控制正则化强度的超参数，$\\frac{1}{2}\\lambda W^{2}$的系数$\\frac{1}{2}$是用于将$\\frac{1}{2}\\lambda W^{2}$的求导结果变成$\\lambda W^{2}$的调整用常量)\n",
    "   - L2范数相当于各个元素的平方和。除了L2范数，还有L1范数(`各个元素的绝对值之和`)，L$\\infty$范数(`各个元素绝对值种最大的那一个`)。\n",
    "   - 使用L1范数，是为了找到稀疏向量。降低权重值，最终获得一个较小的数。也有利于特征选择，因为有时我们可能遇到好几百上千种特征，L1范数可以帮助我们找到重要的那些特征，然后将其余的变成0\n",
    "   - 使用L2范数，它不支持稀疏向量，因为它倾向于保持所有权重一致较小，用L2范数训练模型，会使模型效果更好。\n",
    "3. Dropout\n",
    "   - Dropout方法是为了解决随着模型的负责都不断提高，权重衰减方法越加难以应付的问题的一个新方法。Dropout方法是一种在学习的过程中随机删除神经元的方法。训练时，每传递一次数据，就会随机的选出隐藏层的神经元，然后将其删除，然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.局部最低点与梯度消失\n",
    "1. 局部最低点是梯度下降法的局限性\n",
    "![梯度下降的局限性](./limit_gradient_descent_algorithm.jpg)\n",
    "<br>\n",
    "2. 梯度消失是sigmoid函数的局限性<br>\n",
    "根据sigmoid函数图像不难看出，其越往两端走，导数越小，不断趋近于0，一堆很小的数相乘，最终得到数就更小了，那么我们使用梯度下降时，每次变动就很小，最终可能很难到达一个局部最低点。\n",
    "![梯度下降的局限性](./limit_sigmoid_function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.批次与随机梯度下降(`Batch Gradient Descent & Stochastic Gradient Descent`)\n",
    "1. 考虑一个问题，如果数据量很大，在一个多层感知机的神经网络中，走完一个步长需要占用很大的内存，很长的时间，那么有没有一个方法可以加速这个过程呢？批次与随机梯度下降就出现了！\n",
    "2. 如果一组数据分布比较合理，那么一小部分数据就可以告诉我们梯度是多少，虽然不是最精确的梯度计算方法，但速度很快。\n",
    "3. 随机梯度下降的原理也很简单。取出一小部分数据，让他们经历整个神经网络，算出误差函数的梯度，然后沿着该方向移动一个步长。\n",
    "4. 综上，使用批次梯度下降对数据分组，然后使用随机梯度下降对每一组数据进行反向传播，最终就能又快又好地训练出一个模型。\n",
    "5. 注意，虽然我们对数据执行了n个步长，但是对于普通梯度下降，我们仅对所有数据执行了一个步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.解决局部最低点问题\n",
    "1. 随即重新开始\n",
    "   - 从几个随机的不同地点开始，对所有这些点执行梯度下降处理，这样就增大了抵达全局最低点，或者是非常低的局部最低点的概率。\n",
    "2. 动量(`momentum`)\n",
    "   - 如果在局部最低点卡住，可以用动量和决心更快的移动，翻过驼峰找到更低的局部最低点。\n",
    "   - 动量是一个介于(0, 1)之间的常量$\\beta$,它与步长的关系如下：\n",
    "     - 上一步乘以1，再上一步乘以$\\beta$，再上一步乘以$\\beta ^{2}$，再上一步乘以$\\beta ^{3}$，以此类推。\n",
    "     - 这样，很久以前作用的步长，就比很近的步长作用要小，这样就可以翻过驼峰\n",
    "     - 当我们到达全局最低点时，依然会超过一点点，但程度不大\n",
    "     \n",
    "![动量](./momentum.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.Keras 优化程序\n",
    "#### SGD\n",
    "这是随机梯度下降。它使用了以下参数：\n",
    "- 学习速率。\n",
    "- 动量（获取前几步的加权平均值，以便获得动量而不至于陷在局部最低点）。\n",
    "- Nesterov 动量（当最接近解决方案时，它会减缓梯度）。\n",
    "\n",
    "#### Adam\n",
    "Adam (Adaptive Moment Estimation) 使用更复杂的指数衰减，不仅仅会考虑平均值（第一个动量），并且会考虑前几步的方差（第二个动量）。\n",
    "\n",
    "#### RMSProp\n",
    "RMSProp (RMS 表示均方根误差）通过除以按指数衰减的平方梯度均值来减小学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 11.神经网络回归\n",
    "- 之前使用神经网络都是分析分类问题，那如果分析回归问题呢？只需要删除最后一个sigmoid函数单元，就可以得到前面各层的输出加权总和。为了训练这个模型，我们使用不同的误差函数，给定均方误差函数或标签与预测之间的方差，即$(y - \\hat{y})^{2}$。结合反向传播，我们就可以像分类一样训练这个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 三.卷积神经网络(`Convolutional Neural Networks`，**CNN**)\n",
    "CNN的框架和神经网络大致相同，都可以像搭积木一样通过组装层来构建。不过，CNN中新出现了卷积层(`Convolution层`)和池化层(`Pooling层`)。\n",
    "![cnn结构](./cnn_structural.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
