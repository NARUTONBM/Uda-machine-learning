{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.神经网络(`Neural Networks`)\n",
    "不妨先简单地将神经网络理解为：能自动从数据中学习到合适的权重的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 1.分类问题\n",
    "神经网络是机器学习中的一种模型，可以用于两类问题的解答：\n",
    "   - 分类：把数据划分成不同的类别\n",
    "   - 回归：建立数据间的连续关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.感知机(`Perceptron`)\n",
    "- 一般而言，`朴素感知机`是指单层网络，指的是激活函数使用了阶跃函数的模型；`多层感知机`是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。\n",
    "---\n",
    "1. 感知机接收多个输入，输出一个结果。其结果只有`Positive/Negative(1/0)`两种取值。通常将感知机的方程用向量法简写为：$$Wx + b = 0\\\\W = (w_{1}, w_{2}, \\cdots)\\\\x = (x_{1}, x_{2}, \\cdots)\\\\y = label:0 or 1))\\\\PREDICTION:\\\\ \\hat{y} = \\begin{cases} 1, &Wx + b \\geq0\\\\ 0, &Wx + b < 0\\end{cases}$$\n",
    "   - 对于更高维度(具有多于两个特征)的模型，一样是可以简化成：$$Wx + b = 0$$的方程\n",
    "   - 激活函数(`activation function`)：将输入信号的总和转换成输出信号的函数。激活函数是连接感知机和神经网络的桥梁。\n",
    "   - 阶跃函数(`step function`)：激活函数以阈值为界，一旦输入超过阈值，就切换输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.感知机算法的实现\n",
    "1. 用随机的权重$w_{1},w_{2}, \\cdots, w_{n}$初始化，得到方程$Wx + b = 0$和正负区域；\n",
    "2. 找到$x_{1}, x_{2}, \\cdots, x_{n}$中所有的分类错误点，然后遍历循环执行以下操作：\n",
    "   - if prediction = 0:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} + \\alpha * x_{i}$\n",
    "      - $b = b + \\alpha$\n",
    "   - if prediction = 1:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} - \\alpha * x_{i}$\n",
    "      - $b = b - \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.误差函数(`Error Function`)\n",
    "1. 误差函数提供给我们的预测值与实际值之间的差异，通过寻找最小的误差函数值来找到与实际值误差最小的预测值，从而更新权重。\n",
    "2. 在简单的线性方程中，我们可以通过判断“预测值与实测值相比是大了还是小了”来决定权重是增加还是减少。但是在更为复杂的非线性环境中呢？答案至一就是：\n",
    "3. 梯度下降法(`gradient descent method`)：通过不断地沿梯度方向前进，逐渐减小函数值的过程。\n",
    "4. 对于优化，误差函数选择连续型(`continued`)比离散型(`discrete`)要好。需要从离散型预测变成连续型预测。离散型得到就是`yes`或`no`(1/0)，而连续型得到的将是一个介于(0,1)之间的数字，我们可以视之为概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.Sigmoid函数\n",
    "$$\n",
    "h(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "---\n",
    "1. sigmoid函数与阶跃函数的不同：\n",
    "    - `平滑性`的不同。sigmoid是一条平滑的曲线，输出随着输入发生连续的变化；而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。\n",
    "    - sigmoid函数可以返回(0,1)之间的实数，而阶跃函数只能返回(0/1)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.Softmax函数\n",
    "$$\n",
    "y_{k} = \\frac{e^{a_{k}}}{\\sum_{i=0}^{n} e^{a_{i}}}\n",
    "$$\n",
    "---\n",
    "1. softmax函数的输出受到每个输入信号的影响。\n",
    "2. softmax函数的缺点：其在实现中要进行指数运算，但是此时指数函数的值很容易变得非常大，很容易造成`溢出问题`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.One-Hot 编码\n",
    "    计算机在表示多结果的分类时，使用One-Hot编码是比较常见的处理方式。\n",
    "---\n",
    "![应用示例](./one_hot_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.最大似然率(`Maximum Likeihood`)\n",
    "    一组事件，每个事件在模型中真实发生的概率的乘积。通过比较不同的模型的概率乘积，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.交叉熵(`Cross Entropy`)\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n} y * \\ln^{p} + (1-y) * \\ln^{1-p}\n",
    "$$\n",
    "---\n",
    "1. 对8中所提到的每个事件发生的概率求其ln值，再求其相反数的和，这个和就是交叉熵。\n",
    "2. 交叉熵越小，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.多分类交叉熵\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}*\\ln^{p_{ij}}\n",
    "$$\n",
    "---\n",
    "![应用示例](./muti-class_cross-entropy_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 11.Logistic(对数几率)回归\n",
    "    对数几率算法是所有机器学习的基石\n",
    "---\n",
    "1. 基本流程\n",
    "    - 获得数据\n",
    "    - 选择一个随机模型\n",
    "    - 计算误差\n",
    "    - 最小化误差，获得更好的模型\n",
    "    - 完成！\n",
    "2. 计算误差函数\n",
    "$$\n",
    "error = -y * \\ln^{\\hat{y}} - (1-y) * \\ln^{1-\\hat{y}}\n",
    "$$\n",
    "$$\n",
    "error-function  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\hat{y_{i}}} - (1-y_{i}) * \\ln^{1-\\hat{y_{i}}}\n",
    "$$\n",
    "---\n",
    "PS：图中的log 应为 ln；左侧公式需改为$-\\ln^{0.6} - \\ln^{0.2} - \\ln^{0.1} - \\ln^{0.7}$ = 4.8，并非log(0.6)= - (log0.2) - log(0.1) - log(0.7) = 4.8 ；右侧公式需改为$-\\ln^{0.7} - \\ln^{0.9} - \\ln^{0.9} - \\ln^{0.6}$ = 1.2，并非log(0.7)= - (log0.9) - log(0.9) - log(0.6) = 1.2\n",
    "![应用示例](./logistic_error_sample.jpg)\n",
    "由于$\\hat{y}$是线性方程$Wx + b$的sigmoid函数，误差函数可以写成：\n",
    " $$\n",
    "E(W, b)  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\sigma(Wx^{(i)} + b)} - (1-y_{i}) * \\ln^{1-\\sigma(Wx^{(i)} + b)}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 12.梯度下降的推导过程与实现\n",
    "- 梯度，是误差函数关于权重$(W_{1}, W_{2},\\cdots, W_{n})$和偏差的偏导数所形成的向量。\n",
    "- 梯度下降法就是按照高度(`误差函数`)的梯度负值方向，移动很多次，每一次叫做`epoch`\n",
    "![梯度的实际意义](./gradient_descent_define.jpg)\n",
    "- 多层感知机的梯度计算方法是一样的，只是$\\hat{y}$的方程更加复杂,梯度也几乎一样，只是表达式长了很多：$$\\nabla E_{i,j} = \\frac{\\partial E}{\\partial w_{j}^{(i)}}$$\n",
    "\n",
    "- 推导过程：\n",
    "    首先，sigmoid函数具有完美的导数，即\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1-\\sigma(x))\n",
    "$$\n",
    "  简单推导下：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ba81c06c-40be-4ae9-b557-cc0f74cd4116\" width=\"251px\" class=\"index--image--1wh9w\">\n",
    "现在，如果有$m$个样本点，标为$x^{(1)}, x^{(2)}, \\cdots, x^{(m)}$, 误差公式是：\n",
    "$$\n",
    "E = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\ln(\\hat{y^{(i)}}) + (1-y^{(i)}) \\ln (1-\\hat{y^{(i)}}) \\right)\n",
    "$$\n",
    "预测是$\\hat{y^{(i)}} = \\sigma(Wx^{(i)} + b)$。<br>我们的目标是计算$E$, 在单个样本点 x 时的梯度（偏导数），其中 x 包含 n 个特征，即$x = (x_{1}, \\ldots, x_n)$。\n",
    "$$\n",
    "\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)\n",
    "$$\n",
    "为此，首先我们要计算$\\frac{\\partial}{\\partial w_j} \\hat{y}$.<br>\n",
    "$\\hat{y} = \\sigma(Wx+b),$因此：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cfe9e171-2608-4c05-a1bb-f9a7d1a5eee1\" width=\"752px\" class=\"index--image--1wh9w\">\n",
    "最后一个等式是因为和中的唯一非常量项相对于$w_{j}$正好是$w_{j}x_{j}$,明显具有导数$x_{j}$.<br>\n",
    "现在可以计算$\\frac{\\partial}{\\partial w_j} E$.\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ccfebc74-13ff-48a8-9d8c-3562f5b9945b\" width=\"780px\" class=\"index--image--1wh9w\">\n",
    "类似的计算将得出：<br>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/936e53ac-6b05-436e-bbc9-9f5a01e82a0a\" width=\"185px\" class=\"index--image--1wh9w\">\n",
    "这个实际上告诉了我们很重要的规则。对于具有坐标$(x_1, \\ldots, x_n)$的点，标签$y$, 预测$\\hat{y}$,该点的误差函数梯度是$\\left(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}) \\right)$.<br>\n",
    "总之\n",
    "$$\n",
    "\\nabla E(W,b) = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1).\n",
    "$$\n",
    "如果思考下，会发现很神奇。梯度实际上是标量乘以点的坐标！什么是标量？也就是标签和预测直接的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。\n",
    "![推导过程](./gradient_descent_algorithm_how.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 13.对比Logistic(对数概率)感知器和梯度下降\n",
    "区别在于梯度下降的$\\hat{y}$可以取到(0，1)之间的实数，对数概率的$\\hat{y}$只能取到(0/1).\n",
    "---\n",
    "![对比图](./gradient_descent_algorithm_and_logistic_algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 二.深度神经网络(`Deep Neural Networks`)\n",
    "简单来说，就是多个感知机的叠加，但是输出层可能有多个节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.多层感知机(神经网络架构)\n",
    "1. 多层感知机，顾名思义就是一层一层朴素感知机的叠加，第一层的输入经过其方程处理后得到一个输出，这个输出是第一层的输出，也是第二层的输入，最终我们通过一个激活函数得到最终的输出结果。\n",
    "![多层感知机结构图](./deep_neural_networks_structure.jpg)<br>\n",
    "2. 多层感知机在处理二分类问题时，就如上述所说，那么如果处理多分类问题呢？其实也差不多。我们可以在输出层添加更多的节点，每个节点输出各自的得分，再使用之前提到的Softmax函数算出各自的概率。\n",
    "![多层感知机多分类结构图](./deep_neural_networks_muti_class_structure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.前向反馈(`Feedforward`)\n",
    "1. 表示的是从输入到输出的传递处理。\n",
    "2. 传入输入向量，应用线性模型序列和sigmoid函数，每一层的线性特征图相结合，最终变成高度非线性特征图。最终公式为：$$\\hat y = \\sigma(W^{(2)}\\sigma(W^{(1)}*x))$$\n",
    "![信号传递](./forward_matrix.jpg)\n",
    "图中$W^{(1)}$是3x2矩阵，$x$是3x1矩阵，无法直接相乘，应该使用$W^{(1)}$的转置与$x$相乘<br>\n",
    "3. 多层感知机的前向反馈\n",
    "![多层信号传递](./forward_multi_layer_matrix.jpg)\n",
    "4. 神经网络的误差函数\n",
    "    误差函数依旧可以使用之前的方程，只是$\\hat{y}$的形势更加复杂一些。$$\\hat y = \\sigma(W^{(3)}\\sigma(W^{(2)}\\sigma(W^{(1)}*x)))$$$$E  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\hat{y_{i}}} - (1-y_{i}) * \\ln^{1-\\hat{y_{i}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.后向传播(`Back propagation`)\n",
    "1. 后向反馈包括：\n",
    "    - 进行前向反馈运算。\n",
    "    - 将模型的输出与期望的输出进行比较。\n",
    "    - 计算误差。\n",
    "    - 向后运行前向反馈运算（反向传播），将误差分散到每个权重上。\n",
    "    - 更新权重，并获得更好的模型。\n",
    "    - 继续此流程，直到获得很好的模型。\n",
    "2. 多层感知机的后向传播和朴素感知机的过程是一样的，只是误差函数更加复杂一些。\n",
    "3. 对于多层感知机$\\nabla E$向量的每一项，将$\\frac{\\partial E}{\\partial w_{ij}^{(k)}}$乘以学习速率$\\alpha$，这样就得到了更新后的$W_{ij}^{'(k)}$\n",
    "4. 前向反馈是多种函数的复合，而后向反馈是算出每一步的导数，而根据链式法则，计算复合函数的导数=每一层函数的偏导数相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.用 Keras 构建神经网络\n",
    "要使用 Keras，你需要知道以下几个核心概念。\n",
    "- 序列模型\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    #Create the Sequential model\n",
    "    model = Sequential()\n",
    "```\n",
    "<a target=\"_blank\" href=\"https://keras.io/models/sequential/\">keras.models.Sequential</a> 类是神经网络模型的封装容器。它会提供常见的函数，例如 <code>fit()</code>、<code>evaluate()</code>和<code>compile()</code>。\n",
    "- 层<br>\n",
    "Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 <code>add()</code> 函数添加层。\n",
    "   - 全连接层(`fully-connected`)：相邻层的所有神经元之间都有连接。\n",
    "   - 最大池化层\n",
    "   - 激活层<br>\n",
    "简单的模型可以如下所示：\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "    #创建序列模型\n",
    "    model = Sequential()\n",
    "\n",
    "    #第一层 - 添加有128个节点的全连接层以及32个节点的输入层\n",
    "    model.add(Dense(128, input_dim=32))\n",
    "\n",
    "    #第二层 - 添加 softmax 激活层\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #第三层 - 添加全连接层\n",
    "    model.add(Dense(10))\n",
    "\n",
    "    #第四层 - 添加 Sigmoid 激活层\n",
    "    model.add(Activation('sigmoid'))\n",
    "```\n",
    "Keras 将根据第一层自动推断后续所有层的形状。这意味着，你只需为第一层设置输入维度。<br>\n",
    "上面的第一层<code>model.add(Dense(input_dim=32))</code> 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。<br>\n",
    "构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 <code>categorical_crossentropy</code>。我们还可以指定优化程序，稍后我们将了解这一概念，暂时将使用 <code>adam</code>。最后，我们可以指定评估模型用到的指标。我们将使用准确率。\n",
    "```python\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "```\n",
    "我们可以使用以下命令来查看模型架构：\n",
    "```python\n",
    "    model.summary()\n",
    "```\n",
    "然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。<br>\n",
    "然后使用fit命令训练模型并通过 epoch 参数来指定训练轮数（周期），每 epoch 完成对整数据集的一次遍历。 verbose 参数可以指定显示训练过程信息类型，这里定义为 0 表示不显示信息。\n",
    "```python\n",
    "    model.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "```\n",
    "注意：在 Keras 1 中，<code>nb_epoch</code>会设置 epoch 次数，但是在 Keras 2 中，变成了<code>epochs</code>。<br>\n",
    "最后，我们可以使用以下命令来评估模型：\n",
    "```python\n",
    "    model.evaluate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.早期停止(`Early stopping`)\n",
    "对模型使用梯度下降，直到测试误差停止降低并开始增大，此时停止，这就是早期停止法，广泛应用于训练神经网络。\n",
    "![模型复杂度图表](./early_stopping_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.正则化(`Regularization`)\n",
    "1. 正则化方法的出现，是为了应对过拟合问题。具体的方法有：权值衰减，Dropout等。\n",
    "2. 权值衰减(`Overfit weight decay`)\n",
    "   - 该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。因为很多过拟合就是因为权重参数取值过大而造成。\n",
    "   - 神经网络的学习目的就是为了减小损失函数的值，此时在损失函数加上权重的平方范数(`L2范数`)，这样就可以抑制权重变大。\n",
    "   - 对于权重$W$，L2范数的权重衰减就是$\\frac{1}{2}\\lambda W^{2}$($\\lambda$是控制正则化强度的超参数，$\\frac{1}{2}\\lambda W^{2}$的系数$\\frac{1}{2}$是用于将$\\frac{1}{2}\\lambda W^{2}$的求导结果变成$\\lambda W^{2}$的调整用常量)\n",
    "   - L2范数相当于各个元素的平方和。除了L2范数，还有L1范数(`各个元素的绝对值之和`)，L$\\infty$范数(`各个元素绝对值种最大的那一个`)。\n",
    "   - 使用L1范数，是为了找到稀疏向量。降低权重值，最终获得一个较小的数。也有利于特征选择，因为有时我们可能遇到好几百上千种特征，L1范数可以帮助我们找到重要的那些特征，然后将其余的变成0\n",
    "   - 使用L2范数，它不支持稀疏向量，因为它倾向于保持所有权重一致较小，用L2范数训练模型，会使模型效果更好。\n",
    "3. Dropout\n",
    "   - Dropout方法是为了解决随着模型的负责都不断提高，权重衰减方法越加难以应付的问题的一个新方法。Dropout方法是一种在学习的过程中随机删除神经元的方法。训练时，每传递一次数据，就会随机的选出隐藏层的神经元，然后将其删除，然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.局部最低点与梯度消失\n",
    "1. 局部最低点是梯度下降法的局限性\n",
    "![梯度下降的局限性](./limit_gradient_descent_algorithm.jpg)\n",
    "<br>\n",
    "2. 梯度消失是sigmoid函数的局限性<br>\n",
    "根据sigmoid函数图像不难看出，其越往两端走，导数越小，不断趋近于0，一堆很小的数相乘，最终得到数就更小了，那么我们使用梯度下降时，每次变动就很小，最终可能很难到达一个局部最低点。\n",
    "![梯度下降的局限性](./limit_sigmoid_function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.批次与随机梯度下降(`Batch Gradient Descent & Stochastic Gradient Descent`)\n",
    "1. 考虑一个问题，如果数据量很大，在一个多层感知机的神经网络中，走完一个步长需要占用很大的内存，很长的时间，那么有没有一个方法可以加速这个过程呢？批次与随机梯度下降就出现了！\n",
    "2. 如果一组数据分布比较合理，那么一小部分数据就可以告诉我们梯度是多少，虽然不是最精确的梯度计算方法，但速度很快。\n",
    "3. 随机梯度下降的原理也很简单。取出一小部分数据，让他们经历整个神经网络，算出误差函数的梯度，然后沿着该方向移动一个步长。\n",
    "4. 综上，使用批次梯度下降对数据分组，然后使用随机梯度下降对每一组数据进行反向传播，最终就能又快又好地训练出一个模型。\n",
    "5. 注意，虽然我们对数据执行了n个步长，但是对于普通梯度下降，我们仅对所有数据执行了一个步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.解决局部最低点问题\n",
    "1. 随即重新开始\n",
    "   - 从几个随机的不同地点开始，对所有这些点执行梯度下降处理，这样就增大了抵达全局最低点，或者是非常低的局部最低点的概率。\n",
    "2. 动量(`momentum`)\n",
    "   - 如果在局部最低点卡住，可以用动量和决心更快的移动，翻过驼峰找到更低的局部最低点。\n",
    "   - 动量是一个介于(0, 1)之间的常量$\\beta$,它与步长的关系如下：\n",
    "     - 上一步乘以1，再上一步乘以$\\beta$，再上一步乘以$\\beta ^{2}$，再上一步乘以$\\beta ^{3}$，以此类推。\n",
    "     - 这样，很久以前作用的步长，就比很近的步长作用要小，这样就可以翻过驼峰\n",
    "     - 当我们到达全局最低点时，依然会超过一点点，但程度不大\n",
    "     \n",
    "![动量](./momentum.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.Keras 优化程序\n",
    "#### SGD\n",
    "这是随机梯度下降。它使用了以下参数：\n",
    "- 学习速率。\n",
    "- 动量（获取前几步的加权平均值，以便获得动量而不至于陷在局部最低点）。\n",
    "- Nesterov 动量（当最接近解决方案时，它会减缓梯度）。\n",
    "\n",
    "#### Adam\n",
    "Adam (Adaptive Moment Estimation) 使用更复杂的指数衰减，不仅仅会考虑平均值（第一个动量），并且会考虑前几步的方差（第二个动量）。\n",
    "\n",
    "#### RMSProp\n",
    "RMSProp (RMS 表示均方根误差）通过除以按指数衰减的平方梯度均值来减小学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 11.神经网络回归\n",
    "- 之前使用神经网络都是分析分类问题，那如果分析回归问题呢？只需要删除最后一个sigmoid函数单元，就可以得到前面各层的输出加权总和。为了训练这个模型，我们使用不同的误差函数，给定均方误差函数或标签与预测之间的方差，即$(y - \\hat{y})^{2}$。结合反向传播，我们就可以像分类一样训练这个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 12.超参数(`Hyper-Parameter`)\n",
    "超参数，是指各层的神经元数量、batch大小、参数更新时的学习率和权值衰减等。通过设置合适的超参数，可以使模型性能得到提升。而决定超参数的过程，往往伴随着很多试错，下面是几种高效地寻找超参数的方法。\n",
    "1. 验证数据\n",
    "   - 调整超参数时，必须使用超参数专用的确认数据，而这部分数据就叫做验证数据。原因就在于，当使用测试数据确认超参数的值的“好坏”时，会导致超参数的值被调整为只拟合测试数据，这样的话，就得不到能拟合其他数据、泛化能力低的模型。\n",
    "   - 训练数据用于参数（权重和偏执）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（理想的是只使用一次）测试数据。\n",
    "2. 超参数的优化<br>进行超参数的优化时，逐渐缩小超参数的“好值”的存在范围非常重要。具体步骤如下：\n",
    "   1. 先大致设定超参数的范围；\n",
    "   2. 从设定的超参数范围中随机采样；\n",
    "   3. 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别进度（但是要讲epoch设置的很小）；\n",
    "   4. 重复步骤1和2（100次等），根据它们的识别精度的结果，缩小超参数的范围。\n",
    "   \n",
    "   反复进行上述操作，不断缩小超参数的范围，在缩小到一定范围时，从该范围中选出一个超参数的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 三.卷积神经网络(`Convolutional Neural Networks`，**CNN**)\n",
    "CNN的框架和神经网络大致相同，都可以像搭积木一样通过组装层来构建。不过，CNN中新出现了卷积层(`Convolution层`)和池化层(`Pooling层`)。\n",
    "![cnn结构](./cnn_structural.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. 分类交叉熵(`Categorical Cross-Entropy Loss`)\n",
    "   - 取待选模型中，交叉熵的值最小的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. 局部连接性(`Locally Connected Layer`)\n",
    "   1. 当MLP处理图像识别时会遇到如下两个主要问题：\n",
    "      1. 由于使用了全连接层，即使是很小的28X28的图片，也会导致包含超过50万的参数。很容易导致计算复杂度失控；\n",
    "      2. 由于要将二维图像（矩阵）转换成一维向量，导致数据损失了所有的二维信息，这些空间信息或者说像素之间的位置信息，可以帮助理解图片并非常有助于发现像素值中包含的规律。\n",
    "   2. CNN通过使用更加稀疏互联的层级来解决上述问题：\n",
    "      1. 接受矩阵输入\n",
    "      2. 通过新的细分，并将一小部分像素分配给不同的隐藏节点，每个隐藏节点都会发现对应区域的规律，然后每个隐藏节点再向输出层汇报，输出层将各个隐藏层节点发现的规律结合到一起。也就是局部连接性\n",
    "   3. 局部连接层使用比全连接层更少的参数，更不容易过拟合，并且真正的了解如何发现图片数据中包含的规律。\n",
    "   4. 增加隐藏层中节点数量，可以从数据中发现更加复杂的规律\n",
    "\n",
    "![局部连接层](./locally_connected_structural.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. 卷积层(`Convolution Layer`)\n",
    "   1. 过滤器(`Filter`)，也有叫滤波器或者核。对于输入数据，卷积运算以一定间隔滑动过滤器的窗口并应用。将各个位置上过滤器的元素和输入对应元素相乘，然后再求和，为避免出现复制的情况，我们通常会加上一个`ReLu`函数（这个计算过程也叫乘积求和运算）。然后再将结果保存到输出的对应位置，将这个过程在所有位置都进行一遍，最后再加上偏置（将偏置的值应用到所有输出元素上），就可以得到卷积运算的输出。\n",
    "   2. 当我们使用卷积神经网络时，我们经常会可视化过滤器，得到的可视化图表会告诉我们过滤器会检测到什么样的规律。\n",
    "   3. 数十个、数百个过滤器得到的节点集合就形成了卷积层。\n",
    "   4. 我们把通过过滤器得到的每一个节点成为特征映射(`Feature Maps`)或激活映射(`Activation Maps`)。\n",
    "   5. 和`MLP`一样，在创建`CNN`时，我们始终会指定误差函数，对于多分类问题(`Multiclass Classification`)，将是分类交叉熵损失。然后通过反向传播训练模型，在每个`epoch`都会更新过滤器，以便设定可以最小化损失函数的值。也就是说，`CNN`基于损失函数，确定他需要检测什么样的规律。\n",
    "   6. 过滤器是随机的，不会去指定，指定的是过滤器的数量和大小；规律是学习得到的，也不会去指定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.步幅(`Stride`)和填充(`Padding`)\n",
    "1. 步幅是指过滤器在图片上每次移动的距离（像素数量）。\n",
    "2. 填充是指再进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等）。\n",
    "3. 使用填充主要是为了调整输出的大小。因为在反复进行多次卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.`Keras`中的卷积层\n",
    "1. 要在 Keras 中创建卷积层，你首先必须导入必要的模块：\n",
    "```python\n",
    "   from keras.layers import Conv2D\n",
    "```\n",
    "2. 然后，你可以通过使用以下格式创建卷积层：\n",
    "```python\n",
    "   Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n",
    "```\n",
    "3. 参数\n",
    "   1. 必须传递以下参数：\n",
    "      1. <code>filters</code> - 过滤器数量。\n",
    "      2. <code>kernel_size</code> - 指定（方形）卷积窗口的高和宽的数字。\n",
    "   2. 你可能还需要调整其他可选参数：\n",
    "      1. <code>strides</code> - 卷积 stride。如果不指定任何值，则<code>strides</code>设为<code>1</code>。\n",
    "      2. <code>padding</code> - 选项包括<code>'valid'</code>和<code>'same'</code>。如果不指定任何值，则<code>padding</code>设为<code>'valid'</code>。\n",
    "      3. <code>activation</code> - 通常为<code>'relu'</code>。如果未指定任何值，则不应用任何激活函数。<strong>强烈建议</strong>你向网络中的每个卷积层添加一个`ReLU`激活函数。\n",
    "      4. <strong>注意</strong>：可以将<code>kernel_size</code>和<code>strides</code>表示为数字或元组。\n",
    "      5. 在模型中将卷积层当做第一层级（出现在输入层之后）时，必须提供另一个<code>input_shape</code>参数：\n",
    "         1. <code>input_shape</code> - 指定输入的高度、宽度和深度（按此顺序）的元组。\n",
    "      6. <strong>注意</strong>：如果卷积层<em>不是</em>网络的第一个层级，<strong>请勿</strong>包含<code>input_shape</code> 参数。\n",
    "      7. 还可以设置很多其他元组参数，以便更改卷积层的行为。要详细了解这些参数，建议参阅官方<a target=\"_blank\" href=\"https://keras.io/layers/convolutional/\">文档</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.维度(`Dimension`)\n",
    "1. 和神经网络一样，我们按以下步骤在 Keras 中创建 CNN：首先创建一个<code>序列</code>模型。<br>使用<code>.add()</code>方法向该网络中添加层级。\n",
    "```python\n",
    "   from keras.models import Sequential\n",
    "   from keras.layers import Conv2D\n",
    "\n",
    "   model = Sequential()\n",
    "   model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='valid', \n",
    "        activation='relu', input_shape=(200, 200, 1)))\n",
    "   model.summary()\n",
    "```\n",
    "2. 公式：卷积层中的参数数量<br>\n",
    "   卷积层中的参数数量取决于<code>filters</code>、<code>kernel_size</code>和<code>input_shape</code>的值。我们定义几个变量：<ul>\n",
    "<li><code>K</code> - 卷积层中的过滤器数量 </li>\n",
    "<li><code>F</code> - 卷积过滤器的高度和宽度</li>\n",
    "<li><code>D_in</code> - 上一层级的深度</li>\n",
    "</ul>\n",
    "      \n",
    "      <strong>注意：</strong><code>K</code> = <code>filters</code>，<code>F</code> = <code>kernel_size</code>。类似地，<code>D_in</code>是<code>input_shape</code>元组中的最后一个值。\n",
    "      \n",
    "      因为每个过滤器有<code>F * F * D_in</code>个权重，卷积层由<code>K</code>个过滤器组成，因此卷积层中的权重总数是<code>K * F * F * D_in</code>。因为每个过滤器有 1 个偏差项，卷积层有<code>K</code>个偏差。因此，卷积层中的<strong>参数数量</strong>是<code>K * F * F * D_in + K</code>。\n",
    "3. 公式：卷积层的形状<br>\n",
    "   卷积层的形状取决于<code>kernel_size</code>、<code>input_shape</code>、<code>padding</code>和<code>stride</code>的值。我们定义几个变量：\n",
    "      <ul>\n",
    "        <li><code>K</code> - 卷积层中的过滤器数量</li>\n",
    "        <li><code>F</code> - 卷积过滤器的高度和宽度</li>\n",
    "        <li><code>H_in</code> - 上一层级的高度 </li>\n",
    "        <li><code>W_in</code> - 上一层级的宽度</li>\n",
    "      </ul>\n",
    "      \n",
    "      <strong>注意：</strong><code>K</code> = <code>filters</code>、<code>F</code> = <code>kernel_size</code>，以及<code>S</code> = <code>stride</code>。类似地，<code>H_in</code>和<code>W_in</code>分别是<code>input_shape</code>元组的第一个和第二个值。<br>\n",
    "      卷积层的<strong>深度</strong>始终为过滤器数量 <code>K</code>。\n",
    "      <p>如果 <code>padding = 'same'</code>，那么卷积层的空间维度如下：</p>\n",
    "      <ul>\n",
    "        <li><strong>height</strong> = ceil(float(<code>H_in</code>) / float(<code>S</code>))</li>\n",
    "        <li><strong>width</strong> = ceil(float(<code>W_in</code>) / float(<code>S</code>))</li>\n",
    "      </ul>\n",
    "      <p>如果 <code>padding = 'valid'</code>，那么卷积层的空间维度如下:</p>\n",
    "      <ul>\n",
    "        <li><strong>height</strong> = ceil(float(<code>H_in</code> - <code>F</code> + 1) / float(<code>S</code>))</li>\n",
    "        <li><strong>width</strong> = ceil(float(<code>W_in</code> - <code>F</code> + 1) / float(<code>S</code>))</li>\n",
    "      </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.池化层(`Pooling Layer`)\n",
    "1. 卷积层中，每个过滤器负责从图片中寻找一种规律，过滤器越多，特征映射的堆栈就越大，意味着卷积层的维度可能会很庞大。维度越高，就需要使用越多的参数，就越容易导致过拟合。所以，这也就是池化层在卷积神经网络中扮演的角色，降低维度。下面主要看两种池化；<br>\n",
    "2. 最大池化层(`Max Pooling Layer`)\n",
    "   - 最大池化是以卷积层的特征映射堆栈作为输入，从指定窗口范围内取出最大值。\n",
    "3. 全局平均池化(`Globel Average Pooling`)\n",
    "   - 与最大池化有些不同，既不指定窗口大小，也不指定<code>stride</code>，这是一种更加极端的降维池化类型。同样以卷积层的特征映射堆栈作为输入，但是输出结果是每个特征映射的节点平均值。可以直接将三维数组降维成一维向量。\n",
    "4. 在图像识别领域，最要使用`Max池化`。\n",
    "5. Keras 中的最大池化层\n",
    "   1. 要在 Keras 中创建最大池化层，你必须首先导入必要的模块：\n",
    "   ```python\n",
    "      from keras.layers import MaxPooling2D\n",
    "   ```\n",
    "   2. 然后使用以下格式创建卷积层：\n",
    "   ```python\n",
    "      MaxPooling2D(pool_size, strides, padding)\n",
    "   ```\n",
    "   3. 参数:\n",
    "   <p>你必须包含以下参数：</p>\n",
    "   <ul>\n",
    "      <li><code>pool_size</code> - 指定池化窗口高度和宽度的数字。</li>\n",
    "   </ul>\n",
    "   <p>你可能还需要调整其他可选参数：</p>\n",
    "   <ul>\n",
    "     <li><code>strides</code> - 垂直和水平 stride。如果不指定任何值，则 <code>strides</code> 默认为 <code>pool_size</code>。</li>\n",
    "     <li><code>padding</code> - 选项包括 <code>'valid'</code> 和 <code>'same'</code>。如果不指定任何值，则 <code>padding</code> 设为 <code>'valid'</code>。</li>\n",
    "   </ul>\n",
    "   <p><strong>注意</strong>：可以将 <code>pool_size</code> 和 <code>strides</code> 表示为数字或元组。</p>\n",
    "\n",
    "![](./layers.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.`CNN`模型的建立过程\n",
    "<p>和神经网络一样，我们通过首先创建一个<code>序列</code>模型来创建一个 CNN。</p>\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "```\n",
    "<p>导入几个层，包括熟悉的神经网络层，以及在这节课学习的新的层。</p>\n",
    "\n",
    "```python\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "```\n",
    "<p>和神经网络一样，通过使用 <code>.add()</code> 方法向网络中添加层级：</p>\n",
    "\n",
    "```pyhton\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "<p>该网络以三个卷积层（后面跟着最大池化层）序列开始。前 6 个层级旨在将图片像素数组输入转换为所有空间信息都丢失、仅保留图片内容信息的数组 。然后在 CNN 的第七个层级将该数组扁平化为向量。后面跟着两个密集层，旨在进一步说明图片中的内容。最后一层针对数据集中的每个对象类别都有一个条目，并具有一个<code>softmax</code>激活函数，使其返回概率。</p>\n",
    "<p><strong>注意</strong>：在该视频中，你可能注意到卷积层表示为 <code>Convolution2D</code>，而不是 <code>Conv2D</code>。对于 Keras 2.0 来说，二者都可以，但是最好使用 <code>Conv2D</code>。</p>\n",
    "<h4 id=\"-\">注意事项</h4>\n",
    "<ul>\n",
    "<li>始终向<code>CNN</code>中的 <code>Conv2D</code> 层添加<code>ReLU</code>激活函数。但是网络的最后层级除外，<code>密集</code>层也应该具有<code>ReLU</code>激活函数。</li>\n",
    "<li>在构建分类网络时，网络中的最后层级应该是具有 softmax 激活函数的<code>密集</code>层。最后层级的节点数量应该等于数据集中的类别总数。</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.Keras 中的图片增强功能(`Data Augmentation`)\n",
    "1. 对于输入图片，计算机识别的是一个像素矩阵（彩色图片则是一个三维数组），变换对象的大小、旋转角度或移动在图片中的位置对像素值都会有很大的影响。那么如何才能提升算法的统计不变性。\n",
    "2. 通过随机的平移、旋转或者翻转训练图片得到增强数据，并把它们加入到训练集，通过学习后算法更具有广泛性，测试结果也会更好，还有助于防止过拟合。\n",
    "    1. 创建并配置增强图片生成器\n",
    "\n",
    "    ```python\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    # create and configure augmented image generator\n",
    "    datagen_train = ImageDataGenerator(\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "        horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "    # fit augmented image generator on data\n",
    "    datagen_train.fit(x_train)\n",
    "    ```\n",
    "    2. 可视化原始和增强后的图片\n",
    "\n",
    "    ```python\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # take subset of training data\n",
    "    x_train_subset = x_train[:12]\n",
    "\n",
    "    # visualize subset of training data\n",
    "    fig = plt.figure(figsize=(20,2))\n",
    "    for i in range(0, len(x_train_subset)):\n",
    "            ax = fig.add_subplot(1, 12, i+1)\n",
    "            ax.imshow(x_train_subset[i])\n",
    "    fig.suptitle('Subset of Original Training Images', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    # visualize augmented images\n",
    "    fig = plt.figure(figsize=(20,2))\n",
    "    for x_batch in datagen_train.flow(x_train_subset, batch_size=12):\n",
    "            for i in range(0, 12):\n",
    "                    ax = fig.add_subplot(1, 12, i+1)\n",
    "                    ax.imshow(x_batch[i])\n",
    "            fig.suptitle('Augmented Images', fontsize=20)\n",
    "            plt.show()\n",
    "            break;\n",
    "    ```\n",
    "    3. 定义模型架构\n",
    "\n",
    "    ```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    ```\n",
    "    4. 编译模型\n",
    "\n",
    "    ```python\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    ```\n",
    "    5. 训练模型\n",
    "\n",
    "       - <strong>注意</strong>：使用增强功能的模型在训练时与普通模型的不同点\n",
    "          1. <code>fit</code>变成了<code>fit_generator</code> - 拟合通过<code>ImageDataGenerator</code>类生成的<code>CNN</code>增强图片\n",
    "          2. 对训练集使用<code>flow</code> - 它使数据生成器创建一批批增强图片\n",
    "          3. 我们需要指定一个变量，表示每个<code>epoch</code>的步长数量<code>steps_per_epoch</code>。通常设置为数据集的唯一样本数量除以批次大小<code>batch size</code>\n",
    "\n",
    "    ```python\n",
    "    from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 100\n",
    "\n",
    "    # train the model\n",
    "    checkpointer = ModelCheckpoint(filepath='aug_model.weights.best.hdf5', verbose=1, \n",
    "                                   save_best_only=True)\n",
    "    model.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs, verbose=2, callbacks=[checkpointer],\n",
    "                        validation_data=(x_valid, y_valid),\n",
    "                        validation_steps=x_valid.shape[0] // batch_size)\n",
    "    ```\n",
    "    6. 加载验证准确率最高的权重\n",
    "    \n",
    "    ```python\n",
    "    # load the weights that yielded the best validation accuracy\n",
    "    model.load_weights('aug_model.weights.best.hdf5')\n",
    "    ```\n",
    "    7. 计算测试集的分类准确率\n",
    "    \n",
    "    ```python\n",
    "    # evaluate and print test accuracy\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('\\n', 'Test accuracy:', score[1])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.迁移学习(`Transfer Learning`)\n",
    "<p>迁移学习是指对提前训练过的神经网络进行调整，以用于新的不同数据集。 </p>\n",
    "<p>取决于以下两个条件：</p>\n",
    "<ul>\n",
    "<li>新数据集的大小，以及</li>\n",
    "<li>新数据集与原始数据集的相似程度</li>\n",
    "</ul>\n",
    "<p>使用迁移学习的方法将各不相同。有以下四大主要情形：</p>\n",
    "<ol>\n",
    "<li>新数据集很小，新数据与原始数据相似</li>\n",
    "<li>新数据集很小，新数据不同于原始训练数据</li>\n",
    "<li>新数据集很大，新数据与原始训练数据相似</li>\n",
    "<li>新数据集很大，新数据不同于原始训练数据</li>\n",
    "</ol>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/3fae647e-287e-452b-9cba-c902b63e22a6\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>使用迁移学习的四大情形</center>\n",
    "<p>大型数据集可能具有 100 万张图片。小型数据集可能有 2000 张图片。大型数据集与小型数据集之间的界限比较主观。对小型数据集使用迁移学习需要考虑过拟合现象。 </p>\n",
    "<p>狗的图片和狼的图片可以视为相似的图片；这些图片具有共同的特征。鲜花图片数据集不同于狗类图片数据集。 </p>\n",
    "<p>四个迁移学习情形均具有自己的方法。在下面的几节内容中，我们将分别查看每个情形。</p>\n",
    "<h4 id=\"-\">演示网络</h4>\n",
    "<p>为了解释每个情形的工作原理，我们将以一个普通的预先训练过的卷积神经网络开始，并解释如何针对每种情形调整该网络。我们的示例网络包含三个卷积层和三个完全连接层：</p>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/2fcc5caf-46c3-4915-ac20-9696960fb9b7\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>神经网络的一般概述</center>\n",
    "<p>下面是卷积神经网络的作用一般概述： </p>\n",
    "<ul>\n",
    "<li>第一层级将检测图片中的边缘</li>\n",
    "<li>第二层级将检测形状</li>\n",
    "<li>第三个卷积层将检测更高级的特征</li>\n",
    "</ul>\n",
    "<p>每个迁移学习情形将以不同的方式使用预先训练过的神经网络。</p>\n",
    "<h4 id=\"-1-\">情形 1：小数据集，相似数据</h4>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/85c5bf9f-6e61-42c2-b084-f73360fc128b\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>情形 1：具有相似数据的小数据集</center>\n",
    "<p>如果新数据集很小，并且与原始训练数据相似：</p>\n",
    "<ul>\n",
    "<li>删除神经网络的最后层级</li>\n",
    "<li>添加一个新的完全连接层，与新数据集中的类别数量相匹配</li>\n",
    "<li>随机化设置新的完全连接层的权重；冻结预先训练过的网络中的所有权重</li>\n",
    "<li>训练该网络以更新新连接层的权重</li>\n",
    "</ul>\n",
    "<p>为了避免小数据集出现过拟合现象，原始网络的权重将保持不变，而不是重新训练这些权重。 </p>\n",
    "<p>因为数据集比较相似，每个数据集的图片将具有相似的更高级别特征。因此，大部分或所有预先训练过的神经网络层级已经包含关于新数据集的相关信息，应该保持不变。</p>\n",
    "<p>以下是如何可视化此方法的方式：</p>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/47d819b4-2472-4969-a068-4125b946a937\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>具有小型数据集和相似数据的神经网络</center>\n",
    "<h4 id=\"-2-\">情形 2：小型数据集、不同的数据</h4>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/3d0fbbd6-c73d-496c-be1c-a63e2f655122\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>情形 2：小型数据集、不同的数据</center>\n",
    "<p>如果新数据集很小，并且与原始训练数据不同：</p>\n",
    "<ul>\n",
    "<li>将靠近网络开头的大部分预先训练过的层级删掉</li>\n",
    "<li>向剩下的预先训练过的层级添加新的完全连接层，并与新数据集的类别数量相匹配</li>\n",
    "<li>随机化设置新的完全连接层的权重；冻结预先训练过的网络中的所有权重</li>\n",
    "<li>训练该网络以更新新连接层的权重</li>\n",
    "</ul>\n",
    "<p>因为数据集很小，因此依然需要注意过拟合问题。要解决过拟合问题，原始神经网络的权重应该保持不变，就像第一种情况那样。</p>\n",
    "<p>但是原始训练集和新的数据集并不具有相同的更高级特征。在这种情况下，新的网络仅使用包含更低级特征的层级。</p>\n",
    "<p>以下是如何可视化此方法的方式：</p>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/298a5f0f-ac69-4581-b009-1a5763bef338\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>具有小型数据集、不同数据的神经网络</center>\n",
    "<h4 id=\"-3-\">情形 3：大型数据集、相似数据</h4>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/a0ea6989-608b-43e1-a571-b25d633513d7\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>情形 3：大型数据集、相似数据</center>\n",
    "<p>如果新数据集比较大型，并且与原始训练数据相似：</p>\n",
    "<ul>\n",
    "<li>删掉最后的完全连接层，并替换成与新数据集中的类别数量相匹配的层级</li>\n",
    "<li>随机地初始化新的完全连接层的权重</li>\n",
    "<li>使用预先训练过的权重初始化剩下的权重 </li>\n",
    "<li>重新训练整个神经网络</li>\n",
    "</ul>\n",
    "<p>训练大型数据集时，过拟合问题不严重；因此，你可以重新训练所有权重。</p>\n",
    "<p>因为原始训练集和新的数据集具有相同的更高级特征，因此使用整个神经网络。</p>\n",
    "<p>以下是如何可视化此方法的方式：</p>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/5813bdee-1d46-4188-88c2-971967496348\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>具有大型数据集、相似数据的神经网络</center>\n",
    "<h4 id=\"-4-\">情形 4：大型数据集、不同的数据</h4>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/09d40fc9-6815-4ce3-9469-cddb99ce07b0\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>情形 4：大型数据集、不同的数据</center>\n",
    "<p>如果新数据集很大型，并且与原始训练数据不同：</p>\n",
    "<ul>\n",
    "<li>删掉最后的完全连接层，并替换成与新数据集中的类别数量相匹配的层级</li>\n",
    "<li>使用随机初始化的权重重新训练网络</li>\n",
    "<li>或者，你可以采用和“大型相似数据”情形的同一策略</li>\n",
    "</ul>\n",
    "<p>虽然数据集与训练数据不同，但是利用预先训练过的网络中的权重进行初始化可能使训练速度更快。因此这种情形与大型相似数据集这一情形完全相同。</p>\n",
    "<p>如果使用预先训练过的网络作为起点不能生成成功的模型，另一种选择是随机地初始化卷积神经网络权重，并从头训练网络。</p>\n",
    "<p>以下是如何可视化此方法的方式：</p>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/061f19fd-25b0-40bf-bacd-4451edcbb20b\" width=\"1920px\" class=\"index--image--1wh9w\">\n",
    "<center>具有大型数据集、不同数据的网络</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 四.癌症检测深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.敏感性与特异性\n",
    "1. 敏感性与特异性\n",
    "   1. <strong>敏感性</strong>和<strong>特异性</strong>虽然与<strong>查准率</strong>和<strong>查全率</strong>相似，但并不相同。其定义如下：<p>在癌症示例中，敏感性和特异性指：</p>\n",
    "<ul>\n",
    "<li>敏感性：在<strong>患有</strong>癌症的所有人中，诊断正确的人有多少？</li>\n",
    "<li>特异性：在<strong>未患</strong>癌症的所有人中，诊断正确的人有多少？</li>\n",
    "</ul>\n",
    "<p>查准率和查全率的定义如下：</p>\n",
    "<ul>\n",
    "<li>查准率：在<strong>被诊断</strong>患有癌症的所有人中，多少人确实<strong>得了癌症</strong>？</li>\n",
    "<li>查全率：在<strong>患有癌症</strong>的所有人中，多少人<strong>被诊断</strong>患有癌症？</li>\n",
    "</ul>\n",
    "<p>从这里可以看出，敏感性就是查全率，但特异性并不是查准率。</p>\n",
    "\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/42742d9e-cc5d-4d1a-b63b-a35afdc29339\" alt=\"\" width=\"1953px\" class=\"index--image--1wh9w\">\n",
    "<p>敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记</p>\n",
    "<ul>\n",
    "<li>TP：（真阳性）被<strong>正确</strong>诊断为患病的病人。</li>\n",
    "<li>TN：（真阴性）被<strong>正确</strong>诊断为健康的健康人。</li>\n",
    "<li>FP：（假阳性）被<strong>错误</strong>诊断为患病的健康人。</li>\n",
    "<li>FN：（假阴性）被<strong>错误</strong>诊断为健康的病人。</li>\n",
    "</ul>\n",
    "<p>那么：</p>\n",
    "$$\n",
    "敏感性（Sensitivity） = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "<p>且</p>\n",
    "$$\n",
    "特异性（Specificity） = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/a1b203b0-62d5-459c-af54-8816018a0b98\" alt=\"敏感性和特异性\" width=\"1308px\" class=\"index--image--1wh9w\">\n",
    "<center>敏感性和特异性</center>\n",
    "<p>查准率和查全率是矩阵的第一行和第一列：</p>\n",
    "$$\n",
    "查全率（Recall） = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "<p>且</p>\n",
    "$$\n",
    "查准率（Precision） = \\frac{TP}{TP + FP} \n",
    "$$\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/9106121a-5fcc-42d3-a800-c6b3b8c42e62\" alt=\"查准率和查全率\" width=\"1216px\" class=\"index--image--1wh9w\">\n",
    "<center>查准率和查全率</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
