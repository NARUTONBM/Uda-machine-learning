{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.神经网络(`Neural Networks`)\n",
    "不妨先简单地将神经网络理解为：能自动从数据中学习到合适的权重的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 1.分类问题\n",
    "神经网络是机器学习中的一种模型，可以用于两类问题的解答：\n",
    "   - 分类：把数据划分成不同的类别\n",
    "   - 回归：建立数据间的连续关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.感知机(`perceptron`)\n",
    "- 一般而言，`朴素感知机`是指单层网络，指的是激活函数使用了阶跃函数的模型；`多层感知机`是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。\n",
    "---\n",
    "1. 感知机接收多个输入，输出一个结果。其结果只有`Positive/Negative(1/0)`两种取值。通常将感知机的方程用向量法简写为：$$Wx + b = 0\\\\W = (w_{1}, w_{2}, \\cdots)\\\\x = (x_{1}, x_{2}, \\cdots)\\\\y = label:0 or 1))\\\\PREDICTION:\\\\ \\hat{y} = \\begin{cases} 1, &Wx + b \\geq0\\\\ 0, &Wx + b < 0\\end{cases}$$\n",
    "   - 对于更高维度(具有多于两个特征)的模型，一样是可以简化成：$$Wx + b = 0$$的方程\n",
    "   - 激活函数(`activation function`)：将输入信号的总和转换成输出信号的函数。激活函数是连接感知机和神经网络的桥梁。\n",
    "   - 阶跃函数(`step function`)：激活函数以阈值为界，一旦输入超过阈值，就切换输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.感知机算法的实现\n",
    "1. 用随机的权重$w_{1},w_{2}, \\cdots, w_{n}$初始化，得到方程$Wx + b = 0$和正负区域；\n",
    "2. 找到$x_{1}, x_{2}, \\cdots, x_{n}$中所有的分类错误点，然后遍历循环执行以下操作：\n",
    "   - if prediction = 0:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} + \\alpha * x_{i}$\n",
    "      - $b = b + \\alpha$\n",
    "   - if prediction = 1:\n",
    "      - for i in (1, n):\n",
    "          - $w_{i} = w_{i} - \\alpha * x_{i}$\n",
    "      - $b = b - \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.误差函数(`error function`)\n",
    "1. 误差函数提供给我们的预测值与实际值之间的差异，通过寻找最小的误差函数值来找到与实际值误差最小的预测值，从而更新权重。\n",
    "2. 在简单的线性方程中，我们可以通过判断“预测值与实测值相比是大了还是小了”来决定权重是增加还是减少。但是在更为复杂的非线性环境中呢？答案至一就是：\n",
    "3. 梯度下降法(`gradient descent method`)：通过不断地沿梯度方向前进，逐渐减小函数值的过程。\n",
    "4. 对于优化，误差函数选择连续型(`continued`)比离散型(`discrete`)要好。需要从离散型预测变成连续型预测。离散型得到就是`yes`或`no`(1/0)，而连续型得到的将是一个介于(0,1)之间的数字，我们可以视之为概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.Sigmoid函数\n",
    "$$\n",
    "h(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "---\n",
    "1. sigmoid函数与阶跃函数的不同：\n",
    "    - `平滑性`的不同。sigmoid是一条平滑的曲线，输出随着输入发生连续的变化；而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。\n",
    "    - sigmoid函数可以返回(0,1)之间的实数，而阶跃函数只能返回(0/1)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.Softmax函数\n",
    "$$\n",
    "y_{k} = \\frac{e^{a_{k}}}{\\sum_{i=0}^{n} e^{a_{i}}}\n",
    "$$\n",
    "---\n",
    "1. softmax函数的输出受到每个输入信号的影响。\n",
    "2. softmax函数的缺点：其在实现中要进行指数运算，但是此时指数函数的值很容易变得非常大，很容易造成`溢出问题`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.One-Hot 编码\n",
    "    计算机在表示多结果的分类时，使用One-Hot编码是比较常见的处理方式。\n",
    "---\n",
    "![应用示例](./one_hot_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.最大似然率(`Maximum Likeihood`)\n",
    "    一组事件，每个事件在模型中真实发生的概率的乘积。通过比较不同的模型的概率乘积，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.交叉熵(`Cross Entropy`)\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n} y * \\ln^{p} + (1-y) * \\ln^{1-p}\n",
    "$$\n",
    "---\n",
    "1. 对8中所提到的每个事件发生的概率求其ln值，再求其相反数的和，这个和就是交叉熵。\n",
    "2. 交叉熵越小，模型越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10.多分类交叉熵\n",
    "$$\n",
    "cross-entropy = -\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}*\\ln^{p_{ij}}\n",
    "$$\n",
    "---\n",
    "![应用示例](./muti-class_cross-entropy_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 11.Logistic(对数几率)回归\n",
    "    对数几率算法是所有机器学习的基石\n",
    "---\n",
    "1. 基本流程\n",
    "    - 获得数据\n",
    "    - 选择一个随机模型\n",
    "    - 计算误差\n",
    "    - 最小化误差，获得更好的模型\n",
    "    - 完成！\n",
    "2. 计算误差函数\n",
    "$$\n",
    "error = -y * \\ln^{\\hat{y}} - (1-y) * \\ln^{1-\\hat{y}}\n",
    "$$\n",
    "$$\n",
    "error-function  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\hat{y_{i}}} - (1-y_{i}) * \\ln^{1-\\hat{y_{i}}}\n",
    "$$\n",
    "---\n",
    "PS：图中的log 应为 ln；左侧公式需改为$-\\ln^{0.6} - \\ln^{0.2} - \\ln^{0.1} - \\ln^{0.7}$ = 4.8，并非log(0.6)= - (log0.2) - log(0.1) - log(0.7) = 4.8 ；右侧公式需改为$-\\ln^{0.7} - \\ln^{0.9} - \\ln^{0.9} - \\ln^{0.6}$ = 1.2，并非log(0.7)= - (log0.9) - log(0.9) - log(0.6) = 1.2\n",
    "![应用示例](./logistic_error_sample.jpg)\n",
    "由于$\\hat{y}$是线性方程$Wx + b$的sigmoid函数，误差函数可以写成：\n",
    " $$\n",
    "E(W, b)  = -\\frac{1}{m}\\sum_{i=1}^{n}-y_{i} * \\ln^{\\sigma(Wx^{(i)} + b)} - (1-y_{i}) * \\ln^{1-\\sigma(Wx^{(i)} + b)}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 12.实现梯度下降\n",
    "- 首先，sigmoid函数具有完美的导数，即\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1-\\sigma(x))\n",
    "$$\n",
    "  简单推导下：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ba81c06c-40be-4ae9-b557-cc0f74cd4116\" width=\"251px\" class=\"index--image--1wh9w\">\n",
    "现在，如果有$m$个样本点，标为$x^{(1)}, x^{(2)}, \\cdots, x^{(m)}$, 误差公式是：\n",
    "$$\n",
    "E = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\ln(\\hat{y^{(i)}}) + (1-y^{(i)}) \\ln (1-\\hat{y^{(i)}}) \\right)\n",
    "$$\n",
    "预测是$\\hat{y^{(i)}} = \\sigma(Wx^{(i)} + b)$。<br>我们的目标是计算$E$, 在单个样本点 x 时的梯度（偏导数），其中 x 包含 n 个特征，即$x = (x_{1}, \\ldots, x_n)$。\n",
    "$$\n",
    "\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)\n",
    "$$\n",
    "为此，首先我们要计算$\\frac{\\partial}{\\partial w_j} \\hat{y}$.<br>\n",
    "$\\hat{y} = \\sigma(Wx+b),$因此：\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cfe9e171-2608-4c05-a1bb-f9a7d1a5eee1\" width=\"752px\" class=\"index--image--1wh9w\">\n",
    "最后一个等式是因为和中的唯一非常量项相对于$w_{j}$正好是$w_{j}x_{j}$,明显具有导数$x_{j}$.<br>\n",
    "现在可以计算$\\frac{\\partial}{\\partial w_j} E$.\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/ccfebc74-13ff-48a8-9d8c-3562f5b9945b\" width=\"780px\" class=\"index--image--1wh9w\">\n",
    "类似的计算将得出：<br>\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/936e53ac-6b05-436e-bbc9-9f5a01e82a0a\" width=\"185px\" class=\"index--image--1wh9w\">\n",
    "这个实际上告诉了我们很重要的规则。对于具有坐标$(x_1, \\ldots, x_n)$的点，标签$y$, 预测$\\hat{y}$,该点的误差函数梯度是$\\left(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}) \\right)$.<br>\n",
    "总之\n",
    "$$\n",
    "\\nabla E(W,b) = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1).\n",
    "$$\n",
    "如果思考下，会发现很神奇。梯度实际上是标量乘以点的坐标！什么是标量？也就是标签和预测直接的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 13.梯度下降算法的推导与实现\n",
    "![推导过程](./gradient_descent_algorithm_how.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 14.对比Logistic(对数概率)感知器和梯度下降\n",
    "区别在于感知器的$\\hat{y}$可以取到(0，1)之间的实数，梯度下降的$\\hat{y}$只能取到(0/1).\n",
    "---\n",
    "![对比图](./gradient_descent_algorithm_and_logistic_algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 二.深度神经网络(`Deep Neural Networks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
