{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Foundation in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 一.线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.基础概念\n",
    "1. __标量__(`scalar`)：一个变量就是一个单独的数\n",
    "2. __向量__(`vector`)：一个向量就是一列数，这些数表示在各个轴上的长度，可以表示为带方向的标量。\n",
    "  $$\\vec a =\n",
    "   \\begin{bmatrix}\n",
    "   a1 & a2 & a3 & \\cdots & an\n",
    "   \\end{bmatrix}\n",
    "  $$\n",
    "3. __矩阵__(`matrix`)：一个矩阵就是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。可以理解为多个向量的叠加，一行就是一个向量。我们通常会赋予矩阵粗体的大写变量名称，比如**A** 。\n",
    "  $$A=\n",
    "   \\begin{bmatrix}\n",
    "   A_{1,1} & A_{1,2}\\\\\n",
    "   A_{2,1} & A_{2,2}\n",
    "   \\end{bmatrix}\n",
    "  $$\n",
    "4. __张量__(`tensor`)：一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。我们使用字体 **A** 来表示张量'A', 坐标为 (i, j, k) 的元素记作$ A_{i,j,k}$。\n",
    "5. __转置__(`transpose`)：矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为**主对角线**（`main diagonal`）。矩阵的转置表示为**$A^{\\top}$** ，定义如下：$$(A^{\\top})_{i,j} = A_{j,i}$$向量可以看作只是一列的矩阵。对应的向量的转置可以看作是只有一行的矩阵的转置。有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其变为标准的列向量，来定义一个向量，比如$x = [x_{1}, x_{2}, x_{3}]^{\\top}$。<br>标量可以看作是只有一个元素的矩阵。因此，标量的转置等于它本身，$a = a^{\\top}$。\n",
    "   - 矩阵的转置可以看成以主对角线为轴的一个镜像\n",
    "6. 在深度学习中，我们也使用一些不那么常规的符号。我们允许矩阵和向量相加，产生另一个矩阵：$C = A + b$，其中 $C_{i,j} = A_{i,j} + b_j$。换言之，向量 $b $和矩阵 $A$ 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 $b$ 复制到每一行而生成的矩阵。这种隐式地复制向量 $b$ 到很多位置的方式，被称为__广播__（`broadcasting`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.矩阵和向量相乘\n",
    "1. __矩阵乘积__（`Matrix Product`）规定：矩阵 $A$ 的列数必须和矩阵 $B$ 的行数相等。如果矩阵 $A$ 的形状是 $m\\times n$，矩阵 $B$ 的形状是 $n \\times p$，那么矩阵 $C$ 的形状是 $m \\times p$。我们可以通过将两个或多个矩阵并列放置以书写矩阵乘法，例如$$C=AB$$该乘法操作表示如下：$$C_{i,j}=\\sum_{k}A_{i,k}B_{k,j}$$\n",
    "2. __元素对应乘积__（`Element-Wise Product`）或__Hadamard乘积__（`Hadamard Product`）规定：两个矩阵中对应元素的乘积，记为$A\\bigodot B$\n",
    "3. __点积__（`Dot Product`）：两个相同位数的向量 $x$ 和 $y$ 的点积可看作是矩阵乘积 $x^Ty$。可以把矩阵乘积 $C=AB$ 中计算 $C_{i,j}$ 的步骤看作是 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列之间的点积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.单位矩阵和逆矩阵\n",
    "1. __单位矩阵__（`Identity Matrix`）：规定任意向量和单位矩阵相乘，都不会改变。单位矩阵的结构很简单：所有沿主对角线的元素都是1，而所有其他位置的元素都是0。将保持 $n$ 维向量不变的单位矩阵记作 $I_n$。形式上，$I_n\\in \\mathbb{R}^{n \\times n}$，$$\\forall x \\in \\mathbb{R}^n, I_nx=x$$\n",
    "2. 矩阵 $A$ 的__逆矩阵__（`Matrix Inversion`）记作 $A^{-1}$，期定义的矩阵满足如下条件 $A^{-1}A=I_n$。在求解 $Ax = b$ 时，代入上式，推到可得：$$x = A^{−1}b$$逆矩阵 $A^{−1}$ 主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵 $A^{−1}$ 在数字计算机上只能表现出有限的精度，有效使用向量 $b$ 的算法通常可以得到更精确的 $x$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.线性相关和生成子空间\n",
    "1. __线性组合__（`Linear Combination`）：一组向量中每个向量乘以对应标量系数之后的和，即：$$\\sum_{i}c_iv^{(i)}$$\n",
    "2. __生成子空间__（`Span`）：原始向量线性组合后所能抵达的点的集合。\n",
    "3. __线性相关__（`Linear Dependence`）：存在不全为零的系数，使得 $\\sum_{i}c_iv^{(i)}=0$，称 $A$ 线性相关，否则线性无关，即当且仅当 $c_i$ 全为0时，$\\sum_{i}c_iv^{(i)}=0$ 成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.范数（`Norm`）\n",
    "1. 范数：一种用来衡量向量大小的函数。形式上，$L^p$ 范数定义如下：$$\\lVert x \\rVert _p = \\left( \\sum_{i}\\lvert x_i\\rvert ^p \\right)^{\\frac{1}{p}}$$其中$p \\in \\mathbb{R}, p\\geqslant 1$。范数（包括 $L_p$ 范数）是将向量映射到非负值的函数。直观上来说，向量 $x$ 的范数衡量从原点到点 $x$ 的距离。更严格地说，范数是满足下列性质的任意函数：\n",
    "   1. $f(x)=0\\Rightarrow x=0$\n",
    "   2. $f(x+y)\\leqslant f(x)+f(y)$\n",
    "   3. $\\forall \\alpha \\in \\mathbb{R},f(\\alpha x)=\\lvert \\alpha \\rvert f(x)$\n",
    "2. 当 $p = 2$ 时，$L^2$ 范数被称为__欧几里得范数__（Euclidean norm）。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。$L^2$ 范数在机器学习中出现地十分频繁，经常简化表示为 $\\lVert x\\rVert$，略去了下标 2。平方 $L^2$ 范数也经常用来衡量向量的大小，可以简单地通过点积 $x^⊤x $计算。\n",
    "3. $L^1$ 范数可以简化如下：$$\\lVert x\\rVert=\\sum_i\\lvert x_i\\rvert$$当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 $L^1$ 范数。每当 $x$ 中某个元素从 0 增加 $\\epsilon$，对应的 $L^1$ 范数也会增加 $\\epsilon$。\n",
    "4. 另外一个经常在机器学习中出现的范数是 $L^{\\infty}$ 范数，也被称为__最大范数__（`maxnorm`）。这个范数表示向量中具有最大幅值的元素的绝对值：$$\\lVert x \\rVert_{\\infty}=max_i\\lvert x \\rvert$$\n",
    "5. 有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用__Frobenius 范数__（`Frobenius norm`）$$\\lVert A \\rVert _F = \\sqrt{\\sum_{i,j}A_{i,j}^2}$$类似于向量的 $L^2$ 范数\n",
    "6. 两个向量的__点积__（`dot product`）可以用范数来表示。$$x^{\\top}y = \\lVert x \\rVert _2 \\lVert y \\rVert _2cos\\theta$$其中$\\theta$表示 $x$ 和 $y$ 之间的夹角。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 二.概率与信息论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 三.数值计算"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
