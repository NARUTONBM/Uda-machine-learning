{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Foundation in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 一.线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.基础概念\n",
    "1. __标量__(`scalar`)：一个变量就是一个单独的数\n",
    "2. __向量__(`vector`)：一个向量就是一列数，这些数表示在各个轴上的长度，可以表示为带方向的标量。\n",
    "  $$\\vec a =\n",
    "   \\begin{bmatrix}\n",
    "   a1 & a2 & a3 & \\cdots & an\n",
    "   \\end{bmatrix}\n",
    "  $$\n",
    "3. __矩阵__(`matrix`)：一个矩阵就是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。可以理解为多个向量的叠加，一行就是一个向量。我们通常会赋予矩阵粗体的大写变量名称，比如**A** 。\n",
    "  $$A=\n",
    "   \\begin{bmatrix}\n",
    "   A_{1,1} & A_{1,2}\\\\\n",
    "   A_{2,1} & A_{2,2}\n",
    "   \\end{bmatrix}\n",
    "  $$\n",
    "4. __张量__(`tensor`)：一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。我们使用字体 **A** 来表示张量'A', 坐标为 (i, j, k) 的元素记作$ A_{i,j,k}$。\n",
    "5. __转置__(`transpose`)：矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为**主对角线**（`main diagonal`）。矩阵的转置表示为**$A^{\\top}$** ，定义如下：$$(A^{\\top})_{i,j} = A_{j,i}$$向量可以看作只是一列的矩阵。对应的向量的转置可以看作是只有一行的矩阵的转置。有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其变为标准的列向量，来定义一个向量，比如$x = [x_{1}, x_{2}, x_{3}]^{\\top}$。<br>标量可以看作是只有一个元素的矩阵。因此，标量的转置等于它本身，$a = a^{\\top}$。\n",
    "   - 矩阵的转置可以看成以主对角线为轴的一个镜像\n",
    "6. 在深度学习中，我们也使用一些不那么常规的符号。我们允许矩阵和向量相加，产生另一个矩阵：$C = A + b$，其中 $C_{i,j} = A_{i,j} + b_j$。换言之，向量 $b $和矩阵 $A$ 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 $b$ 复制到每一行而生成的矩阵。这种隐式地复制向量 $b$ 到很多位置的方式，被称为__广播__（`broadcasting`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.矩阵和向量相乘\n",
    "1. __矩阵乘积__（`Matrix Product`）规定：矩阵 $A$ 的列数必须和矩阵 $B$ 的行数相等。如果矩阵 $A$ 的形状是 $m\\times n$，矩阵 $B$ 的形状是 $n \\times p$，那么矩阵 $C$ 的形状是 $m \\times p$。我们可以通过将两个或多个矩阵并列放置以书写矩阵乘法，例如$$C=AB$$该乘法操作表示如下：$$C_{i,j}=\\sum_{k}A_{i,k}B_{k,j}$$\n",
    "2. __元素对应乘积__（`Element-Wise Product`）或__Hadamard乘积__（`Hadamard Product`）规定：两个矩阵中对应元素的乘积，记为$A\\bigodot B$\n",
    "3. __点积__（`Dot Product`）：两个相同位数的向量 $x$ 和 $y$ 的点积可看作是矩阵乘积 $x^Ty$。可以把矩阵乘积 $C=AB$ 中计算 $C_{i,j}$ 的步骤看作是 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列之间的点积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.单位矩阵和逆矩阵\n",
    "1. __单位矩阵__（`Identity Matrix`）：规定任意向量和单位矩阵相乘，都不会改变。单位矩阵的结构很简单：所有沿主对角线的元素都是1，而所有其他位置的元素都是0。将保持 $n$ 维向量不变的单位矩阵记作 $I_n$。形式上，$I_n\\in \\mathbb{R}^{n \\times n}$，$$\\forall x \\in \\mathbb{R}^n, I_nx=x$$\n",
    "2. 矩阵 $A$ 的__逆矩阵__（`Matrix Inversion`）记作 $A^{-1}$，期定义的矩阵满足如下条件 $A^{-1}A=I_n$。在求解 $Ax = b$ 时，代入上式，推到可得：$$x = A^{−1}b$$逆矩阵 $A^{−1}$ 主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵 $A^{−1}$ 在数字计算机上只能表现出有限的精度，有效使用向量 $b$ 的算法通常可以得到更精确的 $x$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.线性相关和生成子空间\n",
    "1. __线性组合__（`Linear Combination`）：一组向量中每个向量乘以对应标量系数之后的和，即：$$\\sum_{i}c_iv^{(i)}$$\n",
    "2. __生成子空间__（`Span`）：原始向量线性组合后所能抵达的点的集合。\n",
    "3. __线性相关__（`Linear Dependence`）：存在不全为零的系数，使得 $\\sum_{i}c_iv^{(i)}=0$，称 $A$ 线性相关，否则线性无关，即当且仅当 $c_i$ 全为0时，$\\sum_{i}c_iv^{(i)}=0$ 成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.范数（`Norm`）\n",
    "1. 范数：一种用来衡量向量大小的函数。形式上，$L^p$ 范数定义如下：$$\\lVert x \\rVert _p = \\left( \\sum_{i}\\lvert x_i\\rvert ^p \\right)^{\\frac{1}{p}}$$其中$p \\in \\mathbb{R}, p\\geqslant 1$。范数（包括 $L_p$ 范数）是将向量映射到非负值的函数。直观上来说，向量 $x$ 的范数衡量从原点到点 $x$ 的距离。更严格地说，范数是满足下列性质的任意函数：\n",
    "   1. $f(x)=0\\Rightarrow x=0$\n",
    "   2. $f(x+y)\\leqslant f(x)+f(y)$\n",
    "   3. $\\forall \\alpha \\in \\mathbb{R},f(\\alpha x)=\\lvert \\alpha \\rvert f(x)$\n",
    "2. 当 $p = 2$ 时，$L^2$ 范数被称为__欧几里得范数__（`Euclidean norm`）。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。$L^2$ 范数在机器学习中出现地十分频繁，经常简化表示为 $\\lVert x\\rVert$，略去了下标 2。平方 $L^2$ 范数也经常用来衡量向量的大小，可以简单地通过点积 $x^⊤x $计算。\n",
    "3. $L^1$ 范数可以简化如下：$$\\lVert x\\rVert=\\sum_i\\lvert x_i\\rvert$$当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 $L^1$ 范数。每当 $x$ 中某个元素从 0 增加 $\\epsilon$，对应的 $L^1$ 范数也会增加 $\\epsilon$。\n",
    "4. 另外一个经常在机器学习中出现的范数是 $L^{\\infty}$ 范数，也被称为__最大范数__（`maxnorm`）。这个范数表示向量中具有最大幅值的元素的绝对值：$$\\lVert x \\rVert_{\\infty}=max_i\\lvert x \\rvert$$\n",
    "5. 有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用__Frobenius 范数__（`Frobenius norm`）$$\\lVert A \\rVert _F = \\sqrt{\\sum_{i,j}A_{i,j}^2}$$类似于向量的 $L^2$ 范数\n",
    "6. 两个向量的__点积__（`dot product`）可以用范数来表示。$$x^{\\top}y = \\lVert x \\rVert _2 \\lVert y \\rVert _2cos\\theta$$其中$\\theta$表示 $x$ 和 $y$ 之间的夹角。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 二.概率与信息论\n",
    "从事件发生的概率认识概率的方法，被称为__频率派概率__（`frequentist probability`）；而根据观测数据假设成立的概率，被称为__贝叶斯概率__（`Bayesian probability`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.概率分布（`Probability Distribution`）\n",
    "1. 随机变量（`Random Variable`）：随机的取不同值的变量。一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。\n",
    "   - 随机变量可以是离散的或者连续的。离散随机变量拥有有限或者可数无限多的状态。注意这些状态不一定非要是整数；它们也可能只是一些被命名的状态而没有数值。连续随机变量伴随着实数值。\n",
    "2. 概率分布用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。描述概率分布的方式取决于随机变量是离散的还是连续的。\n",
    "   1. 离散型变量（`Discrete Variable`）的概率分布用__概率质量函数__（`probability mass function, PMF`）来描述。离散型变量的每个可能的取值都具有大于0小于1的概率。\n",
    "      1. 通常每一个随机变量都会有一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的 __PMF__，而不是根据函数的名称来推断；例如，$P($x$)$ 通常和 $P($y$)$ 不一样。\n",
    "      2. 概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称为 __联合概率分布__（`joint probability distribution`）。$P($x = $x,$ y = $y)$ 表示 x = $x$ 和 y = $y$ 同时发生的概率。我们也可以简写为 $P(x, y)$。\n",
    "   2. 连续型变量（`Continuous Variable`）的概率分布用__概率密度函数__（`probability density function, PDF`）来描述。概率密度函数 $p(x)$ 并没有直接对特定的状态给出概率，相对的，它给出了落在面积为 $\\delta x$ 的无限小的区域内的概率为 $p(x)\\delta x$。\n",
    "      1. 们可以对概率密度函数求积分来获得点集的真实概率质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. 边缘概率分布（`Marginal Probability distribution`）\n",
    "   1. __边缘概率分布__是指对于已知的一组变量的联合概率分布，定义在其子集上的概率分布。\n",
    "   2. 假设有离散型随机变量 x 和 y，并且我们知道 $P$(x, y)。我们可以依据下面的__求和法则__（`sum rule`）来计算 $P$(x)：$$\\forall x \\in {\\rm x},P({\\rm x} =x)= \\sum_{y}P({\\rm x}=x,{\\rm y}=y)$$\n",
    "   3. 对于连续型变量，我们需要用积分替代求和：$$p(x)=\\int p(x,y)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. 条件概率（`Conditional Probability`）\n",
    "   1. __条件概率__是某个事件在给定其他事件发生时出现的概率用下面的公式表示：$$P({\\rm y}=y \\mid {\\rm x}=x)=\\frac{P({\\rm y}=y,{\\rm x}=x)}{P({\\rm x}=x)}$$\n",
    "   2. 条件概率只在 $P({\\rm x} = x) > 0$ 时有定义。我们不能计算给定在永远不会发生的事件上的条件概率。\n",
    "   3. 条件概率的链式法则（`chain rule`）：任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式$$P\\left( {\\rm x}^{(1)},\\ldots,{\\rm x}^{(n)} \\right) = P\\left( {\\rm x}^{(1)} \\right)\\prod_{i=2}^{n} P\\left( {\\rm x}^{(i)} \\mid {\\rm x}^{(1)},\\ldots,{\\rm x}^{(i-1)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. 独立性（`Independence`）和条件独立性（`Conditionally Independent`）\n",
    "   1. 两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是__相互独立的__：$$\\forall x \\in {\\rm x},y \\in {\\rm y},p({\\rm x}=x,{\\rm y}=y)=p({\\rm x}=x)p({\\rm y}=y)$$\n",
    "   2. 如果关于 x 和 y 的条件概率分布对于 z 的每一个值都可以写成乘积的形式，那么这两个随机变量 x 和 y 在给定随机变量 z 时是__条件独立的__：$$\\forall x \\in {\\rm x},y \\in {\\rm y},z \\in {\\rm z},p({\\rm x}=x,{\\rm y}=y \\mid {\\rm z}=z)=p({\\rm x}=x \\mid {\\rm z}=z)p({\\rm y}=y \\mid {\\rm z}=z)$$\n",
    "   \n",
    "    我们可以采用一种简化形式来表示独立性和条件独立性：x$\\perp$y 表示 x 和 y 相互独立，x$\\perp$y $\\mid$ z 表示 x 和 y 在给定 z 时条件独立。\n",
    "   3. 如果联合概率等于两个事件各自概率的乘积，即$P(x,y)=P({\\rm x}=x)P({\\rm y}=y)$，说明这两个事件的发生互不影响，即两者相互独立；对于相互独立事件，条件概率就是自身的概率，即$P({\\rm y}=y \\mid {\\rm x}=x)=P({\\rm y}=y)$\n",
    "   4. 基于条件概率可以得出全概率公式（`law of total probability`）。全概率公式的作用在于将复杂事件的概率求解转化为在不同情况下发生的简单事件的概率求和，即$$P({\\rm x}=x) = \\sum_{i=1}^{n}P({\\rm x}=x \\mid {\\rm y}=y_i)P({\\rm y}=y_i)\\\\ \\sum_{i=1}^{n}P({\\rm y}=y_i)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. 期望（`Expectation`）、方差（`Variance`）和协方差（`Covariance`）\n",
    "   1. 函数 $f(x)$ 关于某分布 $P({\\rm x})$ 的__期望__或者__期望值__（`expected value`）是指，当 $x$ 由 $P$ 产生，$f$ 作用于 $x$ 时，$f(x)$ 的平均值。\n",
    "      1. 对于离散型随机变量，这可以通过求和得到：$$\\mathbb{E}_{{\\rm x} \\sim P}\\left[ f(x) \\right] = \\sum_{x}P(x)f(x)$$\n",
    "      2. 对于连续型随机变量可以通过求积分得到：$$\\mathbb{E}_{{\\rm x} \\sim p}\\left[ f(x) \\right] = \\int p(x)f(x)dx$$\n",
    "      3. 期望是线性的，例如，$$\\mathbb{E}_{{\\rm x}}\\left[ \\alpha f(x) + \\beta g(x) \\right] = \\alpha \\mathbb{E}_{{\\rm x}}\\left[ f(x) \\right] + \\beta \\mathbb{E}_{{\\rm x}} \\left[ g(x) \\right]$$其中 $\\alpha$ 和 $\\beta$ 不依赖于 $x$。\n",
    "   2. __方差__衡量的是当我们对 $x$ 依据它的概率分布进行采样时，随机变量 x 的函数值会呈现多大的差异：$${\\rm Var}\\left( f(x) \\right) = \\mathbb{E}\\left[ \\left( f(x) - \\mathbb{E}\\left[ f(x) \\right] \\right)^2 \\right]$$当方差很小时，$f(x)$ 的值形成的簇比较接近它们的期望值。方差的平方根被称为__标准差__（`standard deviation`）。\n",
    "   3. __协方差__在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：$${\\rm Cov}\\left( f(x),g(y) \\right) = \\mathbb{E}\\left[ \\left( f(x) - \\mathbb{E}\\left[ f(x) \\right] \\right) \\left( g(y) - \\mathbb{E}\\left[ g(y) \\right] \\right) \\right]$$如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。如果协方差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，反之亦然。\n",
    "      - 随机向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$ 的__协方差矩阵__（`covariance matrix`）是一个 $n \\times n$ 的矩阵，并且满足$$\\text{Cov}(\\boldsymbol{{\\rm x}})_{i,j} = \\text{Cov}({\\rm x}_i,{\\rm x}_j)$$协方差矩阵的对角元是方差：$$\\text{Cov}({\\rm x}_i,{\\rm x}_j) = \\text{Var}({\\rm x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.常用概率分布\n",
    "重要的离散分布包括两点分布（`Bernoulli Distribution`）、二项分布（`Binomial Distribution`）和泊松分布（`Poisson Distribution`），重要的连续分布则包括均匀分布（`Uniform Distribution`）、指数分布（`Exponential Distribution`）和正态分布（`Normal Distribution`）。\n",
    "1. __两点分布__：单个二值随机变量的分布。事件发生/不发生的概率分别为 $p/(1-p)$\n",
    "2. __Multinoulli 分布__（`multinoulli distribution`）或者__范畴分布__（`categorical distribution`）是指在具有 $k$ 个不同状态的单个离散型随机变量上的分布，其中 $k$ 是一个有限值。Multinoulli 分布是__多项式分布__（`multinomial distribution`）的一个特例。多项式分布 $\\lbrace 0, \\ldots , n \\rbrace^k$ 中的向量的分布，用于表示当对 Multinoulli 分布采样 $n$ 次时 $k$ 个类中的每一个被访问的次数。Multinoulli 分布由向量 $\\boldsymbol{p} \\in [0, 1]^{k−1}$ 参数化，其中每一个分量 $p_i$ 表示第 $i$ 个状态的概率。最后的第 $k$ 个状态的概率可以通过 $1 − \\boldsymbol{1^{\\top}p}$ 给出。注意我们必须限制 $\\boldsymbol{1^{\\top}p} \\leqslant 1$。Multinoulli 分布经常用来表示对象分类的分布，所以我们很少假设状态 1 具有数值 1 之类的。因此，通常不需要去计算 Multinoulli 分布的随机变量的期望和方差。\n",
    "3. __二项分布__：将满足参数为 p 的两点分布的随机试验独立重复 n 次，事件发生的次数即满足参数为 (n,p) 的二项分布。二项分布的表达式可以写成$$P(X = k) = C^n_k \\cdot p ^ k \\cdot (1 - p) ^ {(n - k)}, 0 \\le k \\le n$$\n",
    "4. __泊松分布__：放射性物质在规定时间内释放出的粒子数所满足的分布，参数为 $\\lambda$ 的泊松分布表达式为 $$P(X = k) = \\lambda ^ k \\cdot e ^ {-\\lambda} / (k!)$$当二项分布中的 n 很大且 p 很小时，其概率值可以由参数为 $\\lambda = np$ 的泊松分布的概率值近似。\n",
    "5. __均匀分布__：在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。\n",
    "6. __指数分布__：满足参数为 $\\theta$ 指数分布的随机变量只能取正值，其概率密度函数为 $\\frac{e^{-\\frac{x}{\\theta}}}{\\theta}, x > 0$。指数分布的一个重要特征是无记忆性：即 $P(X > s + t \\mid X > s) = P(X > t)$。\n",
    "7. __正态分布__：也称为__高斯分布__（`Gaussian Distribution`）$$ \\mathcal{N}(x;\\mu,\\sigma^2) = \\sqrt{\\frac{1}{2\\pi\\sigma^2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^2}(x - \\mu^2) \\right)$$正态分布由两个参数控制，$\\mu \\in \\mathbb{R} 和 \\sigma \\in (0, \\infty)$。参数 $\\mu$ 给出了中心峰值的坐标，这也是分布的均值：$\\mathbb{E}[{\\rm x}]=\\mu$。分布的标准差用 $\\sigma$ 表示，方差用 $\\sigma^2$ 表示。当 $\\mu = 0, \\sigma = 1$ 时，上式称为标准正态分布。正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。\n",
    "8. 分布的混合\n",
    "   1. 通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组合方法是构造__混合分布__（`mixture distribution`）。混合分布由一些组件 (`component`)分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个 Multinoulli 分布中采样的结果：$$P({\\rm x}) = \\sum_{i}P(c=i)P({\\rm x} \\mid c=i)$$这里 $P(c)$ 是对各组件的一个 Multinoulli 分布。\n",
    "   2. 一个非常强大且常见的混合模型是__高斯混合模型__（`Gaussian Mixture Model`），它的组件 $p({\\rm x} \\mid c = i)$ 是高斯分布。每个组件都有各自的参数，均值 $\\boldsymbol{\\mu}^{(i)}$ 和协方差矩阵 $\\boldsymbol{\\sum}^{(i)}$。有一些混合可以有更多的限制。例如，协方差矩阵可以通过 $\\boldsymbol{\\sum}^{(i)}=\\boldsymbol{\\sum}$，$\\forall i$ 的形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制每个组件的协方差矩阵为对角的或者各向同性的 (标量乘以单位矩阵）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.贝叶斯规则（`Bayes's rule`）\n",
    "1. 我们经常会需要在已知 $P({\\rm y} \\mid {\\rm x})$ 时计算 $P({\\rm x} \\mid {\\rm y})$。幸运的是，如果还知道 $P({\\rm x})$，我们可以用__贝叶斯规则__来实现这一目的：$$P({\\rm x} \\mid {\\rm y}) = \\frac{P({\\rm x})P({\\rm y} \\mid {\\rm x})}{P({\\rm y})}$$\n",
    "   1. $P({\\rm x})$ 被称为先验概率（`prior probability`），即预先设定的假设条件成立的概率\n",
    "   2. $P({\\rm y} \\mid {\\rm x})$ 被称为似然概率（`likelihood probability`），即在假设成立的前提下观测到结果的概率\n",
    "   3. $P({\\rm x} \\mid {\\rm y})$ 被称为后验概率（`posterior probability`），即在观测到结果的前提下假设成立的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.常用函数的有用性质\n",
    "某些函数在处理概率分布时经常会出现，尤其是深度学习的模型中用到的概率分布。\n",
    "1. 其中一个函数是 __logistic sigmoid__ 函数：$$\\sigma(x) = \\frac{1}{1+\\text{exp}(-x)}$$logistic sigmoid 函数通常用来产生 Bernoulli 分布中的参数 $\\phi$，因为它的范围是(0, 1)，处在 $\\phi$ 的有效取值范围内。<img src=\"sigmoid_function.png\" width=\"70%\">上图给出了 sigmoid 函数的图示。sigmoid 函数在变量取绝对值非常大的正值或负值时会出现__饱和__（`saturate`）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。\n",
    "2. 另外一个经常遇到的函数是 __softplus__ 函数（`softplus function`）：$$\\zeta(x)=\\text{log}\\left( 1+\\text{exp}(x) \\right)$$softplus 函数可以用来产生正态分布的 $\\beta$ 和 $\\sigma$ 参数，因为它的范围是 (0, $\\infty$)。当处理包含 sigmoid 函数的表达式时它也经常出现。<img src=\"softplus_function.png\" width=\"68%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
