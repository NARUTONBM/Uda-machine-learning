{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 一、强化学习框架：问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.强化学习框架（`RL框架`）\n",
    "1. __RL框架__是指智能体（`Agent`）学习如何与环境互动。<br>假设时间会流逝并离散化时间步（`Time Steps`），在一开始的时间步中，只能体会观察环境，可以把这种结果看成环境呈现给智能体的情形，然后他必须选择合适的响应动作（`Action In Response`）。在下一个时间步对智能体的动作做出响应时，环境向智能体呈现新的情形，同时环境向智能体提供一个奖励，作为回应，智能体必须做出一个动作表示智能体对环境是否做出了正确的响应。这一流程继续下去，在每个时间步，环境都向智能体发送一个观察结果和奖励；作为回应，智能体必须选择一个动作。<br>通常，我们不需要假设环境向智能体显示做出合理决策所需的一切信息。但是如果这样假设，数学会大大简化，在这章节的课程中，假设智能体能够完全观察环境所处的任何状态。此时，智能体接受的不再是观察结果，而是环境状态。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cfff4572-b080-48ab-8872-a96d45468761\" alt=\"\" width=\"738px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.阶段性任务（`Episodic Task`）与连续性任务（`Continuing Task`）\n",
    "1. 在强化学习任务中，具有清晰结束点的任务被称为，阶段性任务。将从头到尾的一系列完整互动称为一个阶段，当一个阶段结束，智能体根据获得的奖励总量，判断自己的表现，然后重头再开始，不过会带有前一次的任务记忆，使得每一次的表现越来越好。经历足够多的次数的训练后，智能体将会得出一个获得奖励非常高的策略。\n",
    "2. 相应的，将没有清晰结束点的任务称为，连续性任务。智能体一直存活，需要学习选择动作的最佳方式同时与环境不断互动，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.奖励假设（`Reward Hypothsis`）\n",
    "- 在机器学习中有个重要的定义假设，即智能体的目标始终可以描述为最大化期望累计奖励（`Maximizing Expected Cumulative Reward`），这就是奖励假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.动作（`Action`）、目标（`Goal`）和奖励（`Reward`）\n",
    "1. 动作 - 智能体在每一个时间步根据环境，判断得出的响应；\n",
    "2. 状态 - 提供给智能体的环境信息，使其能作出合理的动作；\n",
    "3. 目标 - 最大化累计奖励；\n",
    "4. 奖励 - 环境对智能体做出的动作的反馈，表示智能体是否做出了正确的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.折扣回报（`Discounted Return`）\n",
    "1. 更改目标，更关注近期奖励，而不是遥远的未来的奖励。在随机时间步$t$，定义一个折扣率（`Discount Rate`）$\\gamma \\in \\lbrack 0,1\\rbrack$，则回报/折扣回报：$$G_{t} = R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+\\ldots = \\sum_{k=0}^\\infty \\gamma^{k}R_{t+k+1}$$\n",
    "   - 如果$\\gamma = 0$，智能体只关心最即时的奖励。\n",
    "   - 如果$\\gamma = 1$，回报没有折扣。\n",
    "   - $\\gamma$的值越大，智能体越关心遥远的未来。$\\gamma$的值越小，折扣程度越大，在最极端的情况下，智能体只关心最即时的奖励。\n",
    "2. 使用折扣是为了避免无限未来带来的不良影响。\n",
    "3. 当智能体在随机时间步选择动作时，就要用到折扣，这样程序就会更关注于获得更早出现的奖励，而不是稍后出现并且可能性较低的奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.马尔可夫决策流程（`MDP - Markov Decision Process`）\n",
    "1. 一步动态特性（`One-Step Dynamic Feature`）\n",
    "   - 在随机时间步$t$，智能体环境互动变成一系列的状态、动作和奖励。$$(S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t)$$当环境在时间步$t+1$对智能体做出响应时，它只考虑上一个时间步$(S_t,A_t)$的状态和动作。尤其是，它不关心再上一个时间步呈现给智能体的状态。（<em>换句话说</em>，环境不考虑任何$\\lbrace S_0,\\ldots,S_{t-1}\\rbrace$）。<br>并且，它不考虑智能体在上个时间步之前采取的动作。（<em>换句话说</em>，环境不考虑任何$\\{ A_0, \\ldots, A_{t-1}\\}$）。<br>此外，智能体的表现如何，或收集了多少奖励，对环境选择如何对智能体做出响应没有影响。（<em>换句话说</em>，环境不考虑任何$\\{ R_0, \\ldots, R_t \\}$）。<br>因此，我们可以通过指定以下设置完全定义环境如何决定状态和奖励$$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$$对于每个可能的$s', r, s, \\text{and } a$。这些条件概率用于指定环境的<strong>一步动态特性</strong>。\n",
    "2. MDP\n",
    "   - 一组状态$\\mathcal{S}$\n",
    "      - 在阶段性任务中，我们使用$\\mathcal{S}^{+}$表示所有状态集合，包括终止状态。\n",
    "   - 一组动作$\\mathcal{A}$\n",
    "   - 一组奖励$\\mathcal{R}$\n",
    "   - 一步动态特性$$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$$\n",
    "   - 折扣率$\\gamma \\in [0,1]$\n",
    "\n",
    "![](./definition_MDP.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
