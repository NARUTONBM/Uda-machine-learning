{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 一、强化学习框架：问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.强化学习框架（`RL框架`）\n",
    "1. __RL框架__是指智能体（`Agent`）学习如何与环境互动。<br>假设时间会流逝并离散化时间步（`Time Steps`），在一开始的时间步中，只能体会观察环境，可以把这种结果看成环境呈现给智能体的情形，然后他必须选择合适的响应动作（`Action In Response`）。在下一个时间步对智能体的动作做出响应时，环境向智能体呈现新的情形，同时环境向智能体提供一个奖励，作为回应，智能体必须做出一个动作表示智能体对环境是否做出了正确的响应。这一流程继续下去，在每个时间步，环境都向智能体发送一个观察结果和奖励；作为回应，智能体必须选择一个动作。<br>通常，我们不需要假设环境向智能体显示做出合理决策所需的一切信息。但是如果这样假设，数学会大大简化，在这章节的课程中，假设智能体能够完全观察环境所处的任何状态。此时，智能体接受的不再是观察结果，而是环境状态。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cfff4572-b080-48ab-8872-a96d45468761\" alt=\"\" width=\"500px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.阶段性任务（`Episodic Task`）与连续性任务（`Continuing Task`）\n",
    "1. 在强化学习任务中，具有清晰结束点的任务被称为，阶段性任务。将从头到尾的一系列完整互动称为一个阶段，当一个阶段结束，智能体根据获得的奖励总量，判断自己的表现，然后重头再开始，不过会带有前一次的任务记忆，使得每一次的表现越来越好。经历足够多的次数的训练后，智能体将会得出一个获得奖励非常高的策略。\n",
    "2. 相应的，将没有清晰结束点的任务称为，连续性任务。智能体一直存活，需要学习选择动作的最佳方式同时与环境不断互动，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.奖励假设（`Reward Hypothsis`）\n",
    "- 在机器学习中有个重要的定义假设，即智能体的目标始终可以描述为最大化期望累计奖励（`Maximizing Expected Cumulative Reward`），这就是奖励假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.动作（`Action`）、目标（`Goal`）和奖励（`Reward`）\n",
    "1. 动作 - 智能体在每一个时间步根据环境，判断得出的响应；\n",
    "2. 状态 - 提供给智能体的环境信息，使其能作出合理的动作；\n",
    "3. 目标 - 最大化累计奖励；\n",
    "4. 奖励 - 环境对智能体做出的动作的反馈，表示智能体是否做出了正确的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.折扣回报（`Discounted Return`）\n",
    "1. 更改目标，更关注近期奖励，而不是遥远的未来的奖励。在随机时间步$t$，定义一个折扣率（`Discount Rate`）$\\gamma \\in \\lbrack 0,1\\rbrack$，则回报/折扣回报：$$G_{t} = R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+\\ldots = \\sum_{k=0}^\\infty \\gamma^{k}R_{t+k+1}$$\n",
    "   - 如果$\\gamma = 0$，智能体只关心最即时的奖励。\n",
    "   - 如果$\\gamma = 1$，回报没有折扣。\n",
    "   - $\\gamma$的值越大，智能体越关心遥远的未来。$\\gamma$的值越小，折扣程度越大，在最极端的情况下，智能体只关心最即时的奖励。\n",
    "2. 使用折扣是为了避免无限未来带来的不良影响。\n",
    "3. 当智能体在随机时间步选择动作时，就要用到折扣，这样程序就会更关注于获得更早出现的奖励，而不是稍后出现并且可能性较低的奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.马尔可夫决策流程（`MDP - Markov Decision Process`）\n",
    "1. 一步动态特性（`One-Step Dynamic Feature`）\n",
    "   - 在随机时间步$t$，智能体环境互动变成一系列的状态、动作和奖励。$$(S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t)$$当环境在时间步$t+1$对智能体做出响应时，它只考虑上一个时间步$(S_t,A_t)$的状态和动作。尤其是，它不关心再上一个时间步呈现给智能体的状态。（<em>换句话说</em>，环境不考虑任何$\\lbrace S_0,\\ldots,S_{t-1}\\rbrace$）。<br>并且，它不考虑智能体在上个时间步之前采取的动作。（<em>换句话说</em>，环境不考虑任何$\\{ A_0, \\ldots, A_{t-1}\\}$）。<br>此外，智能体的表现如何，或收集了多少奖励，对环境选择如何对智能体做出响应没有影响。（<em>换句话说</em>，环境不考虑任何$\\{ R_0, \\ldots, R_t \\}$）。<br>因此，我们可以通过指定以下设置完全定义环境如何决定状态和奖励$$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$$对于每个可能的$s', r, s, \\text{and } a$。这些条件概率用于指定环境的<strong>一步动态特性</strong>。\n",
    "2. MDP\n",
    "   - 一组状态$\\mathcal{S}$\n",
    "      - 在阶段性任务中，我们使用$\\mathcal{S}^{+}$表示所有状态集合，包括终止状态。\n",
    "   - 一组动作$\\mathcal{A}$\n",
    "   - 一组奖励$\\mathcal{R}$\n",
    "   - 一步动态特性$$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$$\n",
    "   - 折扣率$\\gamma \\in [0,1]$\n",
    "   \n",
    "<img src=\"./definition_MDP.jpg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 二、强化学习框架：解决方案\n",
    "可以将解决方案理解为：智能体为了实现目的必须学会的一系列动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.策略（`Policy`）\n",
    "1. __确定性策略__是从$\\pi: \\mathcal{S}\\to\\mathcal{A}$的映射。对于每个状态$s\\in\\mathcal{S}$，它都生成智能体在状态$s$时将选择的动作$a\\in\\mathcal{A}$\n",
    "2. __随机性策略__是从$\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$的映射。对于每个状态$s\\in\\mathcal{S}$和动作$a\\in\\mathcal{A}$，它都生成智能体在状态$s$时选择动作$a$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.状态值函数（`State-Value Function`）\n",
    "1. 策略$\\pi$的__状态值函数__表示为$v_{\\pi}$。对于每个状态$s \\in\\mathcal{S}$，它都生成智能体从状态$s$开始，然后在所有时间步根据策略选择动作的预期回报。即$$v_\\pi(s) \\doteq \\text{} \\mathbb{E}_\\pi[G_t|S_t=s]$$我们将$v_\\pi(s)$称之为__在策略__$\\pi$__下的状态__$s$__的值__。\n",
    "2. 记法$\\mathbb{E}_\\pi[\\cdot]$来自推荐<a target=\"_blank\" href=\"http://go.udacity.com/rl-textbook\">教科书</a>，其中$\\mathbb{E}_\\pi[\\cdot]$定义为随机变量的预期值（假设智能体遵守策略$\\pi$）。\n",
    "\n",
    "<img src=\"./definition_status_value_function.jpg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.贝尔曼方程-1（`Bellman Expectation Equation`）\n",
    "1. 对于一般__MDP__，我们需要使用期望值，因为通常即时奖励和下个状态无法准确地预测。因为奖励和下个状态是根据__MDP__的一步动态特性选择的。在这种情况下，奖励$r$和下个状态$s'$是从（条件性）概率分布$p(s',r|s,a)$中抽取的，__贝尔曼预期方程（对于$v_{\\pi}$）__表示了任何状态$s$对于_预期即时奖励和下个状态的预期_值的值：$$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "<img src=\"./definition_bellman_expectation_equation.jpg\" width=\"900px\">\n",
    "2. __计算期望值__\n",
    "   1. 如果智能体的策略$\\pi$是<strong>确定性策略</strong>，智能体在状态$s$选择动作$\\pi(s)$，贝尔曼预期方程可以重写为两个变量 ($s'$和$r$）的和：$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma  v_\\pi(s'))$$在这种情况下，我们将奖励和下个状态的折扣值之和$(r+\\gamma  v_\\pi(s'))$与相应的概率$p(s',r|s,\\pi(s))$相乘，并将所有概率相加得出预期值。\n",
    "   2. 如果智能体的策略$\\pi$是<strong>随机性策略</strong>，智能体在状态$s$选择动作$a$的概率是$\\pi(a|s)$，贝尔曼预期方程可以重写为三个变量（$s'$、$r$和$a$）的和：$$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma  v_\\pi(s'))$$在这种情况下，我们将奖励和下个状态的折扣值之和$(r+\\gamma  v_\\pi(s'))$与相应的概率$\\pi(a|s)p(s',r|s,a)$相乘，并将所有概率相加得出预期值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.最优策略（`Optimal Policy`）\n",
    "1. 策略$\\pi '$定义为优于或等同于策略$\\pi$（仅在所有$s\\in\\mathcal{S}$时$v_{\\pi'}(s) \\geq v_\\pi(s)$）。\n",
    "2. __最优策略$\\pi_*$__对于所有策略$\\pi$满足$\\pi_* \\geq \\pi$。最优策略肯定存在，但并不一定是唯一的。\n",
    "3. __最佳策略__正是智能体寻找的策略，是__MDP__的解决方案和实现目标的最佳策略。\n",
    "4. 所有最优策略都具有相同的状态值函数$v_*$，称为__最优状态值函数__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.动作值函数（`Action-Value Function`）\n",
    "1. 动作函数是指状态动作对$s, \\pi(s)$的值是智能体从状态$s$开始并采取动作$\\pi(s)$，然后遵守策略$\\pi$所获得的预期回报。即$$q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$$我们将$q_\\pi(s,a)$称之为__在状态$s$根据策略$\\pi$采取动作$a$的值__（或者称之为__状态动作对__$s,a$的值）。\n",
    "2. 所有最优策略具有相同的动作值函数$q_*$，称之为__最优动作值函数__。\n",
    "3. 对于确定性策略$\\pi,v_\\pi(s) = q_\\pi(s, \\pi(s))$适用于所有$s \\in \\mathcal{S}$。\n",
    "4. 智能体确定最优动作值函数$q_*$后，它可以为所有$s\\in\\mathcal{S}$设置$$\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$$快速获得最优策略。<br>注意，必须确保$v_*(s) = \\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$。\n",
    "5. 如果在某个状态$s\\in\\mathcal{S}$中，有多个动作$a\\in\\mathcal{A}(s)$可以最大化最优动作值函数，你可以通过向任何（最大化）状态分配任意大小的概率构建一个最优策略。只需确保根据该策略给不会最大化动作值函数的动作（对于特定状态）分配的概率是 0% 即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.贝尔曼方程-2\n",
    "1. $q_\\pi$的贝尔曼预期方程是：$$q_\\pi(s,a) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1}) |S_t=s,A_t=a] = \\sum_{s' \\in \\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma\\sum_{a' \\in \\mathcal{A}(s)} \\pi(a'|s') q_\\pi(s',a')) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$$其中最后一个形式详细介绍了如何计算任意随机策略$\\pi$的预期值。该方程表示任何_状态动作对_（根据任意策略）相对于_后续状态_的值（根据同一策略）的值。\n",
    "2. 贝尔曼最优性方程：<br>和贝尔曼预期方程相似，贝尔曼最优性方程可以证明：状态值（以及动作值函数）满足递归关系，可以将状态值（或状态动作对的值）与所有后续状态（或状态动作对）的值联系起来。侧重于最优_策略对应的值满足的关系。\n",
    "   1. __$v_*$的贝尔曼最优性方程__$$v_*(s) = \\max_{a \\in \\mathcal{A}(s)} \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t=s] = \\max_{a \\in \\mathcal{A}(s)}\\sum_{s' \\in \\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_*(s'))$$它表示任何_状态_根据最优策略相对于_后续状态_的值（根据最优策略）的值。\n",
    "   2. __$q_*$的贝尔曼最优性方程__$$q_*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'\\in\\mathcal{A}(S_{t+1})}q_*(S_{t+1},a') | S_t=s, A_t=a]= \\sum_{s' \\in \\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma \\max_{a'\\in\\mathcal{A}(s')}q_*(s',a'))$$它表示任何_状态动作对_根据最优策略相对于_后续状态动作对_（根据最优策略）的值的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 三.动态规划（`Dynamic Programming`）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.迭代方法（`Iterative Method`）\n",
    "1. 为了获得策略$\\pi$对应的状态值函数$v_{\\pi}$，我们只需求解$v_{\\pi}$的贝尔曼预期方程对应的方程组。\n",
    "2. 虽然可以通过分析方式求解方程组，但是通常状态空间非常大，使得直接求解方程组变得非常困难。使用迭代方法简化这一过程。\n",
    "3. 先猜测每个状态的值（通常可以设为0），然后关注某一个状态，再利用状态之间的关系以及贝尔曼期望方程，更新关注的那个状态的值。如此循环下去..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.迭代策略评估算法（`Iterative Policy Evalution Algorithm`）\n",
    "- __迭代策略评估__是在动态规划设置中用到的算法，用于估算策略$\\pi$对应的状态值函数$v_{\\pi}$。在此方法中，我们将向值函数估值中不断循环应用贝尔曼更新，直到估值的变化几乎不易觉察。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/969aeff0-ee0e-4ac3-a0bf-f933f2f55154\" alt=\"\" width=\"427px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.动作值的估算\n",
    "1. 在动态规划设置中，可以使用以下方程从状态值函数$v_{\\pi}$快速获得动作值函数$q_{\\pi}$：$$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$$<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/e2f11231-1c11-4f33-b1ce-17bdfa797617\" alt=\"\" width=\"507px\" class=\"index--image--1wh9w\">\n",
    "2. 在__确定性__环境中，智能体选择某个动作后，下个状态和奖励可以 100% 确定不是随机的。对于确定性环境，所有的$s', r, s, a$为$p(s',r|s,a) \\in \\{ 0,1 \\}$。在这种情况下，当智能体处在状态$s$并采取动作$a$ 时，下个状态$s'$和奖励$r$可以确切地预测，我们必须确保$q_\\pi(s,a) = r + \\gamma v_\\pi(s')$\n",
    "3. 在__随机性__环境中，智能体选择动作后，下个状态和奖励无法确切地预测，而是从<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Conditional_probability_distribution\">（条件性）概率分布</a>$p(s',r|s,a)$中随机抽取的。在这种情况下，当智能体处在状态$s$并采取动作$a$时，每个潜在下个状态$s'$的概率和奖励$r$由$p(s',r|s,a)$确定。在这种情况下，我们必须确保$$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$$，我们计算和$r + \\gamma v_\\pi(s')$的<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Expected_value\">期望值</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.策略改进（`Policy Improvement`）\n",
    "1. 该算法使用策略的值函数，提出一个至少和当前策略一样好的新策略。\n",
    "2. __策略改进__对策略$\\pi$对应的动作值函数$v_{\\pi}$进行估算$V$，并返回改进（或对等）的策略$\\pi'$，其中$\\pi'\\geq\\pi$。该算法首先构建动作值函数估值$Q$。然后，对应每个状态$s\\in\\mathcal{S}$，你只需选择最大化$Q(s,a)$的动作$a$。换句话说，$\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$，针对所有。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/e3fbf474-e22f-4bcd-b5e7-0c2db9240c24\" alt=\"\" width=\"531px\" class=\"index--image--1wh9w\">\n",
    "3. 实现过程：\n",
    "   1. 将值函数转换成动作值函数\n",
    "   2. 对每个状态，选择最大化动作值函数的动作\n",
    "   3. 对于某个状态下，有多个动作可以最大化动作值函数的情况\n",
    "      1. 可以随机选择一个动作\n",
    "      2. 或者构建一个随机性策略，为任何/所有动作分配非零概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.策略迭代（`Policy Iteration`）\n",
    "1. 该算法先对最优策略进行猜测，从对等随机策略（对于每个状态，选择每个动作的概率都是一样的）开始比较合适，通过策略评估获得相应的值函数，再使用该值函数和策略改进，获得一个潜在完善的新策略，然后带入该新策略，再次进行策略评估，然后进行策略改进，不断重复下去，直到收敛于最优策略。<img src=\"policy_iteration.jpg\" width=\"800px\">\n",
    "2. __策略迭代__是一种可以在动态规划设置中解决 MDP 的算法。它包含一系列的策略评估和改进步骤，肯定会收敛于最优策略（对应任意_有限_ MDP）。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/c7ba887a-451a-42b6-8283-cb5d7450f4e6\" alt=\"\" width=\"475px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.截断策略迭代（`Truncated Policy Evaluation`）\n",
    "1. 策略迭代花费的时间，与设置的极小数$\\theta$（$\\theta$越小，时间越长）以及`MDP`的具体情况相关。通过修改终止条件，使得在对状态空间执行固定次数的遍历后，停止评估步骤,此方法称为__截断策略评估__。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/331504f7-cbef-4d0f-b919-5c26bf7ff707\" alt=\"\" width=\"474px\" class=\"index--image--1wh9w\">\n",
    "2. 将这个修订后的策略评估算法应用到类似于策略评估的算法中，称之为__截断策略迭代__。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/a050158f-1833-4ff7-b51c-4af3868965b3\" alt=\"\" width=\"473px\" class=\"index--image--1wh9w\">\n",
    "3. 截断策略迭代的停止条件与策略迭代的不同。在策略迭代中，当策略在一次策略改进步骤之后没有变化时，我们将终止循环。在截断策略迭代中，仅当值函数估值收敛时，我们才停止循环。<img src=\"truncated_policy_iteration.jpg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.值迭代（`Value Iteration`）\n",
    "1. __值迭代__是在动态规划设置中用来估算策略$\\pi$对应的状态值函数$v_\\pi$的算法。对于此方法，每次对状态空间进行遍历时，都同时进行策略评估和策略改进。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/6c0e0ed5-f481-487b-8ff3-159051fce0d2\" alt=\"\" width=\"432px\" class=\"index--image--1wh9w\">\n",
    "2. 如果后续值函数估值之间的差值很小，则满足了停止条件。尤其是，如果对于每个状态，差值都小于$\\theta$，则循环终止。并且，如果我们希望最终值函数估值与最优值函数越接近，则需要将值$\\theta$设得越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 四.蒙特卡洛方法（`Monte_Carlo Method`）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.MC预测：状态值\n",
    "1. 解决预测问题的算法会确定策$\\pi$对应的值函数$v_\\pi$（或$q_\\pi$）。\n",
    "2. 通过与环境互动评估策略$\\pi$的方法分为两大类别：\n",
    "   1. __离线策略方法（`Off-Policy Method`）__使智能体与环境互动时遵守的策略$b$（其中$b\\neq\\pi$）与要评估（或改进）的策略不同。\n",
    "   2. __在线策略方法（`On-Policy Method`）__使智能体与环境互动时遵守的策略$\\pi$与要评估（或改进）的策略相同。\n",
    "3. 状态值是该状态之后的预期回报，因此智能体体验的平均回报是个很好的估值。\n",
    "4. 状态$s\\in\\mathcal{S}$在某个阶段中的每次出现称为$s$的一次经历。\n",
    "5. 有两种类型的蒙特卡洛 (`MC`) 预测方法（用于估算$v_\\pi$）：\n",
    "   1. __首次经历 MC（`First-Visit MC Method`）__ 将$v_\\pi(s)$估算为仅在$s$首次经历之后的平均回报（即忽略与后续经历相关的回报）。\n",
    "   2. __所有经历 MC（`Every-Visit MC Method`）__ 将$v_\\pi(s)$估算为$s$所有经历之后的平均回报。\n",
    "   3. 两者的差别在于：\n",
    "      1. 所有经历 MC 存在<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Bias_of_an_estimator\">偏差</a>，而首次经历 MC 不存在偏差。\n",
    "      2. 一开始，所有经历 MC 具有更低的<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Mean_squared_error\">均方误差 (MSE)</a>，但是随着经历更多的阶段，首次经历 MC 的均方误差更低。\n",
    "   4. 当每个状态的经历次数接近无穷大时，首次经历和所有经历方法都__保证会收敛于__真值函数。（换句话说，只要智能体在每个状态获取足够的经验，值函数估值将非常接近真值。）\n",
    "   \n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/c46f7b7a-8343-4a21-a6e1-9dd8f144d3a5\" width=\"602px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.MC预测：动作值\n",
    "1. 在强化学习设置下并不知道动态特性，为了获得动作值，不再去查看每个状态的经历，而是查看每个潜在动作对的经历，计算每个动作状态对之后的回报，取平均值\n",
    "2. 状态动作对$s,a (s\\in\\mathcal{S},a\\in\\mathcal{A})$在某个阶段中的每次出现称为$s,a$的一次经历。\n",
    "3. 有两种类型的蒙特卡洛 (`MC`) 预测方法（用于估算$q_\\pi$）：\n",
    "   1. __首次经历`MC`__将$q_\\pi(s,a)$估算为仅在$s,a$首次经历之后的平均回报（即忽略与后续经历相关的回报）。\n",
    "   2. __所有经历`MC`__将$q_\\pi(s,a)$估算为$s,a$所有经历之后的平均回报。\n",
    "   \n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/0d911b4b-550c-494d-8324-31f703a93c26\" width=\"602px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.广义策略评估（`Generalized Policy Iteration`）\n",
    "指：不对策略评估周期次数进行限制，并且不对收敛接近程度进行限制的算法。在所有强化学习下都可以使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.MC控制：增量均值\n",
    "1. 假设一个动作状态对经历了一定的次数，将回报表示为$\\lbrace x_1, x_2, \\ldots, x_n\\rbrace$，然后通过对这些值取均值，计算值函数逼近结果，表示为：$$\\mu_n = \\frac{\\sum_{j=1}^{n}x_j}{n}$$\n",
    "2. 在每次经历之后都迭代更新估值，通过推到可以得出第$k$的均值与第$k-1$的均值的关系：$$\\mu_k = \\frac{1}{k}\\sum_{j=1}^{k}x_j\\\\= \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1}x_j)\\\\= \\frac{1}{k}(x_k + (k-1)\\mu_{(k-1)})\\\\= \\mu_{(k-1)} + \\frac{1}{k}(x_k - \\mu_{(k-1)})$$\n",
    "<img src=\"incremental_mean.jpg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.MC控制：策略评估\n",
    "1. 通过对增量均值进行改进，使得能获得多个状态动作对的值函数逼近结果。\n",
    "2. 智能体先从环境中获得一个阶段样例，然后对于每个时间步，查看相应的状态动作对，如果是首次访问，则计算相应的回报，然后根据不断取平均值的算法，更新动作值的相应估值；初始化每个对的经历次数，将上述计算带入蒙特卡洛控制计算中。\n",
    "<img src=\"mc_policy_evaluation.jpg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.MC控制：策略改进\n",
    "1. 如果对于每个状态$s\\in\\mathcal{S}$，它保证会选择满足$a = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$的动作$a\\in\\mathcal{A}(s)$，则策略相对于动作值函数估值$Q$来说是贪婪策略。（通常将所选动作称之为贪婪动作。）\n",
    "2. 如果对于每个状态$s\\in\\mathcal{S}$，策略相对于动作值函数估值$Q$是$\\epsilon$贪婪策略。\n",
    "   1. 概率为$1-\\epsilon$时，智能体选择贪婪动作，以及\n",
    "   2. 概率为$\\epsilon$时，智能体随机（均匀地）选择一个动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.Epsilon 贪婪策略\n",
    "1. 为了构建一个相对于当前动作值函数估值$Q$为$\\epsilon$贪婪策略的策略$\\pi$，我们只需设置<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/cd71ea2c-3df6-4012-b65a-d11a90e60632\" width=\"600px\">针对每个$s\\in\\mathcal{S}$和$a\\in\\mathcal{A}$。注意，$\\epsilon$必须始终是 0-1（含）之间的值（即$\\epsilon \\in [0,1]$）。\n",
    "2. 你可以将遵守$\\epsilon$贪婪策略的智能体看做始终可以操控硬币方向（可能不公平），正面朝上的概率是$\\epsilon$。观察状态后，智能体就会抛掷该硬币。\n",
    "   1. 如果硬币背面朝上（因此概率为$1-\\epsilon$），智能体选择贪婪动作。\n",
    "   2. 如果硬币正面朝上（因此概率为$\\epsilon$），智能体从一组潜在（非贪婪和贪婪）动作中均匀地随机选择一个动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.探索-利用\n",
    "1. 探索-利用困境\n",
    "   - 注意，智能体一开始不知道环境的一步动态特性。为了最大化回报，智能体必须通过互动了解环境。<br>在每个时间步，当智能体选择一个动作时，它都根据以前与环境的互动经验做出决策。并且，为了尽量减少求解`OpenAI Gym`中的环境所需的阶段次数，我们的第一个想法是设计一种策略，其中智能体始终选择它认为（根据过去的经验）将最大化回报的动作。因此，智能体可以遵守对动作值函数估算来说很贪婪的策略。<br>为了了解为何是这种情况，注意在早期阶段，智能体所了解的信息很有限（可能有缺陷）。因此很有可能智能体_估计非贪婪的动作实际上比估计_贪婪的动作更好。<br>因此，成功的强化学习智能体不能在每个时间步都采取贪婪的动作（即它不能始终利用自己的经验）；为了发现最优策略，它必须不断改进所有状态动作对的估算回报（换句话说，它必须继续通过经历每个状态动作对探索各种可能性）。但是，智能体应该始终保持一定的贪婪性，从而_尽快_最大化回报。这就引出了$\\epsilon$贪婪策略。<br>我们将需要平衡这两个竞争性条件的情况称为探索-利用困境。解决该困境的一个潜在方法是在构建$\\epsilon$贪婪策略时逐步修改$\\epsilon$的值。\n",
    "2. 设置$\\epsilon$的值\n",
    "   - 合理的做法是智能体一开始与环境互动时，倾向于探索环境，而不是利用已有的经验。毕竟当智能体对环境的动态特性相对知之甚少时，应该怀疑自己的有限知识并探索环境，或者尝试各种最大化回报的策略。因此，最佳起始策略是等概率随机策略，因为它在每个状态探索所有潜在动作的概率是相同的。你在上个练习中发现，$\\epsilon = 1$会生成一个等同于等概率随机策略的$\\epsilon$贪婪策略。<br>在后续时间步，合理的做法是倾向于利用已有的经验，而不是探索环境，策略在动作值函数估算方面越来越贪婪。毕竟智能体与环境的互动次数越多，它对估算动作值函数的信任就越高。你在上个练习中发现，$\\epsilon = 0$会生成贪婪策略（或者非常倾向于利用已有的经验而不是探索环境的策略）。<br>幸运的是，此策略（一开始倾向于探索环境而不是利用已有的经验，然后逐渐倾向于利用已有的经验而不是探索环境）可以证明是最优策略。\n",
    "3. 有限状态下的无限探索贪婪算法 (`GLIE`)\n",
    "为了保证 MC 控制会收敛于最优策略$\\pi_*$，我们需要确保满足两个条件。我们将这些条件称之为__有限状态下的无限探索贪婪算法 (GLIE)__。尤其是，如果：\n",
    "   - 每个状态动作对$s, a$（针对所有$s\\in\\mathcal{S}$和$a\\in\\mathcal{A}$）被经历无限次;\n",
    "   - 策略收敛相对于动作值函数估算 QQ 来说贪婪的策略。\n",
    "然后，MC 控制保证会收敛于最优策略（在有限状态下运行无穷个阶段）。这些条件确保：\n",
    "   - 智能体继续在所有时间步进行探索\n",
    "   - 智能体逐渐倾向于利用已有的经验（而不是探索环境）。\n",
    "满足这些条件的一种方式是在指定$\\epsilon$贪婪策略时，修改$\\epsilon$的值。尤其是，使$\\epsilon_i$对应于第$i$个时间步。然后在以下情况下，这两个条件都会满足：\n",
    "   - 对于所有时间步$i，\\epsilon_i > 0$\n",
    "   - 当时间步$i$接近无穷大时，$\\epsilon_i$减小到0（即$\\lim_{i\\to\\infty} \\epsilon_i = 0$）。\n",
    "<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/2eba43ac-3b4f-470a-8783-a0c80277c16f\" alt=\"\" width=\"540px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.MC控制：常量$\\alpha$\n",
    "1. 综合前述算法，生成一个阶段，然后对于经历的每个状态动作对$(S_t, A_t)$，都计算接下来的相应回报$G_t$（`Corresponding Return`）,然后再用该回报获取最新的估算，更新步骤表示如下：$$Q(S_t, A_t) \\longleftarrow Q(S_t, A_t) + \\frac{1}{N(S_t,A_t)}(G_t - Q(S_t, A_t))$$\n",
    "2. 对于上述算法，可以这样理解：\n",
    "   1. 计算最近取样的回报$G_t$和对应的状态动作对$(S_t, A_t)$之间的差别，记作：$\\delta_t = G_t - Q(S_t, A_t)$\n",
    "   2. $\\delta_t$是期望回报$Q(S_t, A_t)$与实际回报$G_t$之间的差值\n",
    "      1. $\\delta_t > 0$，实际回报比值函数期望回报要大，说明值函数偏小，因此需要增大估值$Q(S_t, A_t)\\uparrow$\n",
    "      2. $\\delta_t < 0$，实际回报比值函数期望回报要小，说明值函数偏大，因此需要降低预期的动作值函数$Q(S_t, A_t)\\downarrow$\n",
    "      3. 增大/降低的调整幅度与已经经历状态动作对的次数$N(S_t,A_t)$成反比\n",
    "      4. 这种方式，前面的回报对调整影响大，后面的影响小。因此我们需要更改该算法。\n",
    "3. 引入步长参数$\\alpha$，并更新算法：$$Q(S_t, A_t) \\longleftarrow Q(S_t, A_t) + \\alpha(G_t - Q(S_t, A_t))$$\n",
    "   1. 这样可以确保稍后获得的回报比之前获得的回报更受重视，这样，智能体将最信任最新的汇报，并逐渐忘记先前获得的回报。这点非常重要，因为该策略不断改变，每一步都越来越优化，因此实际上后面的时间步对估算动作值来说很重要\n",
    "   2. 步长参数$\\alpha$必须满足$0 < \\alpha \\leq 1$。$\\alpha$值越大，学习速度越快，但是如果$\\alpha$的值过大，可能会导致__MC__控制无法收敛于$\\pi_*$。<img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/4c12506d-c61a-4c54-8f06-18e65940cae0\" alt=\"\" width=\"627px\" class=\"index--image--1wh9w\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
